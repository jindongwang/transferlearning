# Awesome papers by date

Here, we list some papers related to transfer learning by date (starting from 2021-07). For papers older than 2021-07, please refer to the [papers by topic](awesome_paper.md), which contains more papers.

- [Awesome papers by date](#awesome-papers-by-date)
  - [2024-10](#2024-10)
  - [2024-09](#2024-09)
  - [2024-08](#2024-08)
  - [2024-07](#2024-07)
  - [2024-05](#2024-05)
  - [2024-04](#2024-04)
  - [2024-03](#2024-03)
  - [2024-02](#2024-02)
  - [2024-01](#2024-01)
  - [2023-12](#2023-12)
  - [2023-11](#2023-11)
  - [2023-10](#2023-10)
  - [2023-09](#2023-09)
  - [2023-08](#2023-08)
  - [2023-07](#2023-07)
  - [2023-06](#2023-06)
  - [2023-05](#2023-05)
  - [2023-04](#2023-04)
  - [2023-03](#2023-03)
  - [2023-02](#2023-02)
  - [2023-01](#2023-01)
  - [2022-12](#2022-12)
  - [2022-11](#2022-11)
  - [2022-10](#2022-10)
  - [2022-09](#2022-09)
  - [2022-08](#2022-08)
  - [2022-07](#2022-07)
  - [2022-06](#2022-06)
  - [2022-05](#2022-05)
  - [2022-04](#2022-04)
  - [2022-03](#2022-03)
  - [2022-02](#2022-02)
  - [2022-01](#2022-01)
  - [2021-12](#2021-12)
  - [2021-11](#2021-11)
  - [2021-10](#2021-10)
  - [2021-09](#2021-09)
  - [2021-08](#2021-08)
  - [2021-07](#2021-07)

## 2024-10

- Transfer Learning on Multi-Dimensional Data: A Novel Approach to Neural Network-Based Surrogate Modeling [[arxiv](http://arxiv.org/abs/2410.12241)]
  - Transfer learning on multi-dimensioal data

- TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration [[arxiv](http://arxiv.org/abs/2410.12183)]
  - Transfer vision-language models for collaboration

- Test-time adaptation for image compression with distribution regularization [[arxiv](http://arxiv.org/abs/2410.12191)]
  - Test-time adaptation for image compression with distribution regularization

- WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation [[arxiv](http://arxiv.org/abs/2410.12075)]
  - Weather domain generalization

- Can In-context Learning Really Generalize to Out-of-distribution Tasks? [[arxiv](https://arxiv.org/abs/2410.09695)]
  - Can in-context learning generalize to OOD tasks?

- Domain-Conditioned Transformer for Fully Test-time Adaptation [[arxiv](https://arxiv.org/abs/2410.10442)]
  - Fully test-tim adaptation with domain-conditioned transformer

- Safety-Aware Fine-Tuning of Large Language Models [[arxiv](https://arxiv.org/abs/2410.10014)]
  - Fine-tuning with safety in LLMs
- - Deep Transfer Learning: Model Framework and Error Analysis [[arxiv](https://arxiv.org/abs/2410.09383)]
  - Deep transfer learning framework

- Cross-Domain Distribution Alignment for Segmentation of Private Unannotated 3D Medical Images [[arxiv](https://arxiv.org/abs/2410.09210)]
  - Cross-domain adaptation of private unannotated 3D medical images

- Stratified Domain Adaptation: A Progressive Self-Training Approach for Scene Text Recognition [[arxiv](https://arxiv.org/abs/2410.09913)]
  - Stratified domain adaptation

- Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs [[arxiv](https://arxiv.org/abs/2410.08020)]
  - Active fine-tuning of LLMs

- LLM Embeddings Improve Test-time Adaptation to Tabular Y|X shifts [[arxiv](https://arxiv.org/abs/2410.07395)]
  - Test-time adaptation via LLMs

- AHA: Human-Assisted Out-of-Distribution Generalization and Detection [[arxiv](https://arxiv.org/abs/2410.08000)]
  - Human-assisted OOD generalization and detection

## 2024-09

- Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities [[arxiv](https://arxiv.org/abs/2409.07736)]
  - Transfer learning for computer vision survey

- Spatial Adaptation Layer: Interpretable Domain Adaptation For Biosignal Sensor Array Applications [[arxiv](https://arxiv.org/abs/2409.08058)]
  - Interpretable domain adaptation

- DICS: Find Domain-Invariant and Class-Specific Features for Out-of-Distribution Generalization [[arxiv](https://arxiv.org/abs/2409.08557)]
  - Domain-invariant and class-specific features for OOD generalization

- Unsupervised Domain Adaptation Via Data Pruning [[arxiv](https://arxiv.org/abs/2409.12076)]
  - Using pruning for domain adaptation

- LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models [[arxiv](https://arxiv.org/abs/2409.11919)]
  - Black-box adaptation of vision language models

- Can Your Generative Model Detect Out-of-Distribution Covariate Shift? [[arxiv](http://arxiv.org/abs/2409.03043)]
  - Can your generative models detect OOD covariate shift?

- Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities [[arxiv](http://arxiv.org/abs/2409.03444)]
  - Fine-tuning LLMs for domain adaptation

- Dual-Path Adversarial Lifting for Domain Shift Correction in Online Test-time Adaptation [[arxiv](https://arxiv.org/abs/2408.13983)]
  - Online test-time adaptation using dual-path adversarial lifting

- Rethinking Knowledge Transfer in Learning Using Privileged Information [[arxiv](https://arxiv.org/abs/2408.14319)]
  - Using privileged information for knowledge transfer

- Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection [[arxiv](https://arxiv.org/abs/2408.15637)]
  - Transfer learning from simulated to real scens for monocular 3D

- Multi-source Domain Adaptation for Panoramic Semantic Segmentation [[arxiv](https://arxiv.org/abs/2408.16469)]
  - Multi-source domain adaptation for panoramic semantic segmentation

- Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning [[arxiv](https://arxiv.org/abs/2408.16486)]
  - Test-time prompt tuning for open classes

- A More Unified Theory of Transfer Learning [[arxiv](https://arxiv.org/abs/2408.16189)]
  - More unified theory of transfer learning

- Low Saturation Confidence Distribution-based Test-Time Adaptation for Cross-Domain Remote Sensing Image Classification [[arxiv](https://arxiv.org/abs/2408.16265)]
  - Test-time adaptation for remote sensing image classification

## 2024-08

- Unsupervised Domain Adaption Harnessing Vision-Language Pre-training [[arxiv](https://arxiv.org/abs/2408.02192)]
  - Domain adaptation using vision-language pre-training

- Domain penalisation for improved Out-of-Distribution Generalisation [[arxiv](https://arxiv.org/abs/2408.01746)]
  - OOD using domain penalization

- Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift [[arxiv](http://arxiv.org/abs/2407.18428)]
  - Domain generalization under invariant feature shift

## 2024-07

- Reducing Spurious Correlation for Federated Domain Generalization [[arxiv](https://arxiv.org/abs/2407.19174)]
  - Federated domain generalization by reducing spurious correlation

- Can Modifying Data Address Graph Domain Adaptation? [[arxiv](https://arxiv.org/abs/2407.19311)]
  - Alignment and rescaling for graph DA

- Improving Domain Adaptation Through Class Aware Frequency Transformation [[arxiv](https://arxiv.org/abs/2407.19551)]
  - Class aware frequency transformation for domain adaptation

- Rethinking Domain Adaptation and Generalization in the Era of CLIP [[arxiv](http://arxiv.org/abs/2407.15173)]
  - Rethinking domain adaptation in CLIP era

- Training-Free Model Merging for Multi-target Domain Adaptation [[arxiv](http://arxiv.org/abs/2407.13771)]
  - Model merging for multi-target domain adaptation

- SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning [[arxiv](https://arxiv.org/abs/2407.03036)]
  - OOD fine-tuning for foundation models 大模型的OOD微调

- Multi-Task Domain Adaptation for Language Grounding with 3D Objects [[arxiv](https://arxiv.org/abs/2407.02846)]
  - Multi-task domain adaptation for language grounding

## 2024-05

- Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning [[arxiv](https://arxiv.org/abs/2405.11816)]
  - Transfer learning for CSI-based positioning 用迁移学习进行基于CSI的定位

- Versatile Teacher: A Class-aware Teacher-student Framework for Cross-domain Adaptation [[arxiv](https://arxiv.org/abs/2405.11754)]
  - Teacher-student framework for cross-domain adaptation 教师-学生框架进行跨领域适配

- MICCAI'24 MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection [[arxiv](https://arxiv.org/abs/2405.11315)]
  - Adapting clip for few-shot medical image anomaly detection 对CLIP模型进行适配，以用于少样本图片异常检测

## 2024-04

- MDDD: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition [[arxiv](https://arxiv.org/abs/2404.15615)]
  - Manifold-based domain adaptation for EEG-based emotion recognition 基于流形的DA用于EEG情绪识别

- Domain Adaptation for Learned Image Compression with Supervised Adapters [[arxiv](https://arxiv.org/abs/2404.15591)]
  - Domain adaptation for learned image compression DA用于图片压缩

- Test-Time Training on Graphs with Large Language Models (LLMs) [[arxiv](https://arxiv.org/abs/2404.13571)]
  - Test-time training on graphs with LLMs 使用大语言模型在图上进行测试时训练

- DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series [[arxiv](https://arxiv.org/abs/2404.11269)]
  - Domain adaptation for anomaly detection 使用域自适应进行时间序列异常检测

- CVPR'24 Exploring the Transferability of Visual Prompting for Multimodal Large Language Models [[arxiv](https://arxiv.org/abs/2404.11207)]
  - Explore the transferability of visual prompting for multimodal LLM 探索多模态大模型visual prompt tuning的可迁移性

- DGMamba: Domain Generalization via Generalized State Space Model [[arXiv](https://arxiv.org/abs/2404.07794)]
  - Domain generalization using mamba 用Mamba结构进行DG

- CVPR'24 Unified Language-driven Zero-shot Domain Adaptation [[arxiv](https://arxiv.org/abs/2404.07155)]
  - Language-driven zero-shot domain adaptation 语言驱动的零样本 DA

- ICASSP'24 Learning Inference-Time Drift Sensor-Actuator for Domain Generalization [[IEEE](https://ieeexplore.ieee.org/abstract/document/10447537?casa_token=6xrw2hE7cVEAAAAA:9i_ITqbfyLTzQYjdp4Oi16ziD8uheMMZJHRn4gHmzl9nN_j2c5u8MBxUtYYdzlj1Vn4l8F5OJnrw3BY)]
  - Inference-time drift actuator for OOD generalization

- ICASSP'24 SBM: Smoothness-Based Minimization for Domain Generalization [[IEEE](https://ieeexplore.ieee.org/abstract/document/10446613?casa_token=kO10uC18NMQAAAAA:6WJvMr57dSMyORMAnBgFGXi01aE_AmIAA6CQINztT7pHG2u8RmojDxMdV09UO6O9IfFsVEJDrYl1uiU)]
  - Smoothness-based minimization for OOD generalization

- ICASSP'24 G2G: Generalized Learning by Cross-Domain Knowledge Transfer for Federated Domain Generalization [[IEEE](https://ieeexplore.ieee.org/abstract/document/10447043?casa_token=ihJ_LaxqnfUAAAAA:8Petax0UdQ9bvJLrRbFrujWcVjDzIckhYLDvIk-rUZxo-S7pa6xgbGBxkLWjs8c8H1jR4E8Rop8e7cc)]
  - Federated domain generalization

- ICASSP'24 Single-Source Domain Generalization in Fundus Image Segmentation Via Moderating and Interpolating Input Space Augmentation [[IEEE](https://ieeexplore.ieee.org/abstract/document/10447741?casa_token=t0FGpPfYxeoAAAAA:yyZ1zKhXstoaxNOtP6zKBj1ArLF8JZ7gGQOtR-k6DAHCO9SWTIOwLG5TF71BrcenWvO002MYku-wtQI)]
  - Single-source DG in fundus image segmentation

- ICASSP'24 Style Factorization: Explore Diverse Style Variation for Domain Generalization [[IEEE](https://ieeexplore.ieee.org/abstract/document/10447540?casa_token=inLqNDEGEjQAAAAA:7jNUOViyS9PIn-BwIV0LJ-5oCzmM7BXpMLfyLosedaxmxZ-_c_2sA615GlCgrlwaspjdVKa4eogm6Z4)]
  - Style variation for domain generalization

- ICASSP'24 SPDG-Net: Semantics Preserving Domain Augmentation through Style Interpolation for Multi-Source Domain Generalization [[IEEE](https://ieeexplore.ieee.org/abstract/document/10447210?casa_token=NSBeXUg0AdUAAAAA:4rrMR38UcDN2YRzD9Fvm42gT3dyEX5lO0arFkmVIu3VwQLT9UFLAmU3a5ZOfxtr812_Fic1SCcw9mr0)]
  - Domain augmentation for multi-source DG

- ICASSP'24 Domaindiff: Boost out-of-Distribution Generalization with Synthetic Data [[IEEE](https://ieeexplore.ieee.org/abstract/document/10446788?casa_token=Rh3MGM6szOQAAAAA:0GRegU3dIidLVvIYtJb97m2ZDCl0wwKVTmTZH7XTE0fzEBmRuwJHSn_T1U6NgwSYHFPKlWHox_BO4Eg)]
  - Using synthetic data for OOD generalization

- ICASSP'24 Multi-Level Augmentation Consistency Learning and Sample Selection for Semi-Supervised Domain Generalization [[IEEE](https://ieeexplore.ieee.org/abstract/document/10446462?casa_token=vfAJ1GINr0AAAAAA:YS4NVt-kR8-sJqhfo6H7d04ZmckxUUpsIYuy2agnB4IpgCnR7xOzyrNv59MZ2lcbVhNvsN6Cl4p_7YI)]
  - Multi-level augmentation for semi-supervised domain generalization

- ICASSP'24 MMS: Morphology-Mixup Stylized Data Generation for Single Domain Generalization in Medical Image Segmentation [[IEEE](https://ieeexplore.ieee.org/abstract/document/10448305?casa_token=14-2Vm39RD4AAAAA:Zmzm9KTl3INP2I83T2MLwQXtHUZKwXYfhDOPU9F0Eu9SrznInqGpSBrMYH0ek3eemDKdyL4bBU6EVaY)]
  - Morphology-mixup for domain generalization

## 2024-03

- On the Benefits of Over-parameterization for Out-of-Distribution Generalization [[arxiv](http://arxiv.org/abs/2403.17592)]
  - Over-parameterazation for OOD generalizaiton 分析了过参数化对OOD的影响

- CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning [[arxiv](http://arxiv.org/abs/2403.17369)]
  - Chain-of-domain adaptation with visual prompt tuning 领域链adaptation

- Deep Domain Adaptation: A Sim2Real Neural Approach for Improving Eye-Tracking Systems [[arxiv](https://arxiv.org/abs/2403.15947)]
  - Domain adaptation for eye-tracking systems 用DA进行眼球追踪

- EAGLE: A Domain Generalization Framework for AI-generated Text Detection [[arxiv](https://arxiv.org/abs/2403.15690)]
  - Domain generalization for AI content detection 用DG进行AI生成内容检测

- DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization [[arxiv](https://arxiv.org/abs/2403.16697)]
  - Dynamic propmtstyler for source-free DG 动态prompt分格化用于source-free DG

- Neurocomputing'24 Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation [[arxiv](https://arxiv.org/abs/2403.11256)]
  - Unvertainty-aware source-free domain adaptation 基于不确定性伪标签的domain adaptation

- Efficient Domain Adaptation for Endoscopic Visual Odometry [[arxiv](https://arxiv.org/abs/2403.10860)]
  - Efficient domain adaptation for visual odometry 高效DA用于odometry

- Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability [[arxiv](https://arxiv.org/abs/2403.11331)]
  - Domain adaptation in ecology and hydrology 研究生态学和水文学中的DA

- ICLR'24 SF(DA)2: Source-free Domain Adaptation Through the Lens of Data Augmentation [[arxiv](https://arxiv.org/abs/2403.10834)]
  - Source-free DA by data augmentation 通过数据增强来进行source-free DA

- CVPR'24 Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias [[arxiv](https://arxiv.org/abs/2403.11234)]
  - Unviersal semi-supervised DA 通过公共类bias进行半监督DA

- Domain Adaptation Using Pseudo Labels for COVID-19 Detection [[arxiv](https://arxiv.org/abs/2403.11498)]
  - Domain adaptation for COVID-19 detection 用DA进行covid-19检查

- Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans [[arxiv](https://arxiv.org/abs/2403.11338)]
  - Covid-19 test using domain adaptation 使用集成和测试增强用于DA covid-19

- V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions [[arxiv](https://arxiv.org/abs/2403.11371)]
  - DG for multi-agent perception 领域泛化用于极端天气

- Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification [[arxiv](https://arxiv.org/abs/2403.10782)]
  - Bidirectional multi-step DG for REID 双向领域泛化用于REID

- MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks [[arxiv](https://arxiv.org/abs/2403.11646)]
  - Model merge for medical transfer learning 通过模型合并进行医学迁移学习

- SPA: A Graph Spectral Alignment Perspective for Domain Adaptation [[NeurIPS 2023]](https://arxiv.org/abs/2310.17594) [[Pytorch]](https://github.com/CrownX/SPA)
  - Graph spectral alignment and neighbor-aware propagation for domain adaptation

- Addressing Source Scale Bias via Image Warping for Domain Adaptation [[arxiv](https://arxiv.org/abs/2403.12712)]
  - Address the source scale bias for domain adaptation 解决源域的scale bias

- ICLR'24 扩展版 Learning with Noisy Foundation Models [[arxiv](https://arxiv.org/abs/2403.06869)]
  - Fine-tune a noisy foundation model 基础模型有noisy的时候如何finetune

- Visual Foundation Models Boost Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation [[arxiv](https://arxiv.org/abs/2403.10001)]
  - Foundation models help domain adaptation 基础模型帮助领域自适应

- Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling [[arxiv](https://arxiv.org/abs/2403.06978)]
  - Parameter-efficient adaptation for spatiotemporal modeling

- ICASSP'24 Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning [[arxiv](https://arxiv.org/abs/2403.06059)]
  - Test-time distribution learning adapter

- A Study on Domain Generalization for Failure Detection through Human Reactions in HRI [[arxiv](https://arxiv.org/abs/2403.06315)]
  - Domain generalization for failure detection through human reactions in HRI

- ICLR'24 Towards Robust Out-of-Distribution Generalization Bounds via Sharpness [[arxiv](https://arxiv.org/abs/2403.06392)]
  - Robust OOD generalization bounds

- Learning with Noisy Foundation Models [[arxiv](https://arxiv.org/abs/2403.06869)]
  - Learning with noisy foundation models

## 2024-02

- Unsupervised Domain Adaptation within Deep Foundation Latent Spaces [[arxiv](https://arxiv.org/abs/2402.14976)]
  - Domain adaptation using foundation models

## 2024-01

- Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning? [[arxiv](https://arxiv.org/abs/2401.12902)]
  - A comparison between visual prompt tuning and full finetuning 比较prompt tuning和全finetune

- Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy [[arxiv](https://arxiv.org/abs/2401.12129)]
  - OOD detection for ablated learned temperature energy

- LanDA: Language-Guided Multi-Source Domain Adaptation [[arxiv](https://arxiv.org/abs/2401.14148)]
  - Language guided multi-source DA  在多源域自适应中使用语言指导

- AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space [[arxiv](https://arxiv.org/abs/2401.12421)]
  - Semi-spuervised domain adaptation in the embedding space 在嵌入空间中进行半监督域自适应

- Inter-Domain Mixup for Semi-Supervised Domain Adaptation [[arxiv](https://arxiv.org/abs/2401.11453)]
  - Inter-domain mixup for semi-supervised domain adaptation 跨领域mixup用于半监督域自适应

- Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation [[arxiv](https://arxiv.org/abs/2401.10848)]
  - Source-free and image-only unsupervised domain adaptation

- ICLR'24 spotlight Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks [[arxiv](https://arxiv.org/abs/2309.17002)]
  - A new research direction of transfer learning in the era of foundation models 大模型时代一个新研究方向：研究预训练数据的噪声对下游任务影响

- ICLR'24 Supervised Knowledge Makes Large Language Models Better In-context Learners [[arxiv](https://arxiv.org/abs/2312.15918)]
  - Small models help large language models for better OOD 用小模型帮助大模型进行更好的OOD

- NeurIPS'23 Geodesic Multi-Modal Mixup for Robust Fine-Tuning [[paper](https://openreview.net/forum?id=iAAXq60Bw1)]
  - Geodesic mixup for robust fine-tuning

- NeurIPS'23 Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models [[paper](https://openreview.net/forum?id=TPeAmxwPK2)]
  - Parameter and computation efficient transfer learning by reinforcement learning

- NeurIPS'23 Test-Time Distribution Normalization for Contrastively Learned Visual-language Models [[paper](https://openreview.net/forum?id=VKbEO2eh5w)]
  - Test-time distribution normalization for contrastively learned VLM

- NeurIPS'23 A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP) [[paper](https://openreview.net/forum?id=wMNpMe0vp3)]
  - A fine-gained analysis of CLIP robustness

- NeurIPS'23 When Visual Prompt Tuning Meets Source-Free Domain Adaptive Semantic Segmentation [[paper](https://openreview.net/forum?id=ChGGbmTNgE)]
  - Source-free domain adaptation using visual prompt tuning

- NeurIPS'23 CODA: Generalizing to Open and Unseen Domains with Compaction and Disambiguation [[arxiv](https://openreview.net/forum?id=Jw0KRTjsGA)]
  - Open set domain generalization using extra classes

- CPAL'24 FIXED: Frustratingly Easy Domain Generalization with Mixup [[arxiv](https://arxiv.org/abs/2211.05228)]
  - Easy domain generalization with mixup

- SDM'24 Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution [[arxiv](https://arxiv.org/abs/2209.00652)]
  - Optimization and model selection for domain generalization

- Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation [[arxiv](https://arxiv.org/abs/2401.02076)]
  - SAM for single-source domain generalization

- Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition [[arxiv](https://arxiv.org/abs/2401.02344)]
  - Multi-source DA with Transformer-based feature generation

## 2023-12

- Multi-Modal Domain Adaptation Across Video Scenes for Temporal Video Grounding [[arxiv](https://arxiv.org/abs/2312.13633)]
  - Multi-modal domain adaptation 多模态领域自适应

- Domain Adaptive Graph Classification [[arxiv](https://arxiv.org/abs/2312.13536)]
  - Domain adaptive graph classification 域适应的图分类

- Understanding and Estimating Domain Complexity Across Domains [[arxiv](https://arxiv.org/abs/2312.13487)]
  - Understanding and estimating domain complexity 解释领域复杂性

- Prompt-based Domain Discrimination for Multi-source Time Series Domain Adaptation [[arxiv](https://arxiv.org/abs/2312.12276v1)]
  - Prompt-based domain discrimination for time series domain adaptation 基于prompt的时间序列域自适应

- NeurIPS'23 SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models [[arxiv](https://openreview.net/forum?id=EhdNQiOWgQ)]
  - Test-time prompt adaptation for vision language models 对视觉-语言大模型的测试时prompt自适应

- AAAI24 Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects [[arxiv](https://arxiv.org/abs/2312.07374)][[code](https://github.com/jyLin8100/GenSAM)]
  - A training-free test-time adaptation approach to relax the instance-specific prompts requirment in SAM.

- Open Domain Generalization with a Single Network by Regularization Exploiting Pre-trained Features [[arxiv](http://arxiv.org/abs/2312.05141)]
  - Open domain generalization with a single network 用单一网络结构进行开放式domain generalizaition

- Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation [[arxiv](http://arxiv.org/abs/2312.04265)]
  - Using vision foundation models for domain genealized semantic segmentation 用视觉基础模型进行域泛化语义分割

- DARNet: Bridging Domain Gaps in Cross-Domain Few-Shot Segmentation with Dynamic Adaptation [[arxiv](http://arxiv.org/abs/2312.04813)]
  - Dynamic adaptation for cross-domain few-shot segmentation 动态适配用于跨领域小样本分割

- A Unified Framework for Unsupervised Domain Adaptation based on Instance Weighting [[arxiv](http://arxiv.org/abs/2312.05024)]
  - Instance weighting for domain adaptation 样本加权用于领域自适应

- Target-agnostic Source-free Domain Adaptation for Regression Tasks [[arxiv](http://arxiv.org/abs/2312.00540)]
  - Target-agnostic source-free DA for regression 用于回归任务的source-free DA

- On the Out-Of-Distribution Robustness of Self-Supervised Representation Learning for Phonocardiogram Signals [[arxiv](http://arxiv.org/abs/2312.00502)]
  - OOD robustness for self-supervised learning for phonocardiogram 心音图信号自监督的OOD鲁棒性

- Student Activity Recognition in Classroom Environments using Transfer Learning [[arxiv](http://arxiv.org/abs/2312.00348)]
  - Using transfer learning to recognize student activities 用迁移学习来识别学生课堂行为

## 2023-11

- A2XP: Towards Private Domain Generalization [[arxiv](https://arxiv.org/abs/2311.10339)]
  - Private domain generalization 隐私保护的domain generalization

- Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation [[arxiv](http://arxiv.org/abs/2311.05858)]
  - Auto-weighting for test-time adaptation 自动权重的TTA

- Domain Generalization by Learning from Privileged Medical Imaging Information [[arxiv](http://arxiv.org/abs/2311.05861)]
  - Domain generalizaiton by learning from privileged medical imageing inforamtion

- SSL-DG: Rethinking and Fusing Semi-supervised Learning and Domain Generalization in Medical Image Segmentation [[arxiv](https://arxiv.org/abs/2311.02583)]
  - Semi-supervised learning + domain generalization 把半监督和领域泛化结合在一起

- WACV'24 Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization [[arxiv](https://arxiv.org/abs/2311.02599)]
  - Class and domain augmentation for single-source open-domain DG 结合类和domain增强做单源DG

- Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector [[arxiv](https://arxiv.org/abs/2311.02342)]
  - Proposal-level unsupervised domain adaptation

- Robust Fine-Tuning of Vision-Language Models for Domain Generalization [[arxiv](https://arxiv.org/abs/2311.02236)]
  - Robust fine-tuning for domain generalization 用于领域泛化的鲁棒微调

- NeurIPS 2023 Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models [[arxiv](https://arxiv.org/abs/2311.01441)]
  - Distill OOD robustness from vision-language foundational models 从VLM模型中蒸馏出OOD鲁棒性

- UbiComp 2024 Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition [[arxiv](https://arxiv.org/abs/2310.18562)]
  - Test-time adaptation for activity recognition 测试时adaptation用于行为识别

## 2023-10

- PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization [[arxiv](https://arxiv.org/abs/2307.15199)]
  - Prompt-driven style generation for source-free domain generalization

- A Survey of Heterogeneous Transfer Learning [[arxiv](https://arxiv.org/abs/2310.08459v2)]
  - A recent survey of heterogeneous transfer learning 一篇最近的关于异构迁移学习的综述

- Equivariant Adaptation of Large Pre-Trained Models [[arxiv](http://arxiv.org/abs/2310.01647)]
  - Equivariant adaptation of large pre-trained models 对大模型进行等边自适应

- Effective and Parameter-Efficient Reusing Fine-Tuned Models [[arxiv](http://arxiv.org/abs/2310.01886)]
  - Effective and parameter-efficient reusing fine-tuned models 高效使用预训练模型

- Prompting-based Efficient Temporal Domain Generalization [[arxiv](http://arxiv.org/abs/2310.02473)]
  - Prompt based temporal domain generalization 基于prompt的时间域domain generalization

- Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks [[arxiv](https://arxiv.org/abs/2309.17002)]
  - Noisy model learning: fine-tuning to supress the bad effect of noisy pretraining data 通过使用轻量级finetune减少噪音预训练数据对下游任务的影响

- ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning [[arxiv](https://arxiv.org/abs/2310.05143)]
  - Black-box foundation models for personalized federated learning 黑盒的blackbox模型进行个性化迁移学习

## 2023-09

- Domain Generalization with Fourier Transform and Soft Thresholding [[arxiv](http://arxiv.org/abs/2309.09866)]
  - Domain generalization with Fourier transform 基于傅里叶变换和软阈值进行domain generalization

- DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning [[arxiv](http://arxiv.org/abs/2309.05173)]
  - Decomposed prompt tuning for parameter-efficient fine-tuning 基于分解prompt tuning的参数高效微调

- Better Practices for Domain Adaptation [[arxiv](http://arxiv.org/abs/2309.03879)]
  - Better practice for domain adaptation

- Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images [[arxiv](http://arxiv.org/abs/2309.02556)]
  - Domain adaptation for efficient ViT

- Robust Activity Recognition for Adaptive Worker-Robot Interaction using Transfer Learning [[arxiv](http://arxiv.org/abs/2308.14843)]
  - Activity recognition using domain adaptation


## 2023-08

- IJCV'23 Exploring Vision-Language Models for Imbalanced Learning [[arxiv](https://arxiv.org/abs/2304.01457)] [[code](https://github.com/Imbalance-VLM/Imbalance-VLM)]
  - Explore vision-language models for imbalanced learning 探索视觉大模型在不平衡问题上的表现

- ICCV'23 Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning [[arxiv](https://arxiv.org/abs/2308.02533)] [[code](https://github.com/microsoft/robustlearn)]
  - 达到对抗鲁棒性和泛化能力的trade off

- ICCV'23 Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation [[arxiv](https://arxiv.org/abs/2308.14023)]
  - Domain-specificity for source-free DA 用领域特异性驱动的source-free DA

- Unsupervised Domain Adaptation via Domain-Adaptive Diffusion [[arxiv](http://arxiv.org/abs/2308.13893)]
  - Domain-adaptive diffusion for domain adaptation 领域自适应的diffusion

- Multi-Scale and Multi-Layer Contrastive Learning for Domain Generalization [[arxiv](http://arxiv.org/abs/2308.14418)]
  - Multi-scale and multi-layer contrastive learning for DG 多尺度和多层对比学习用于DG

- Exploring the Transfer Learning Capabilities of CLIP in Domain Generalization for Diabetic Retinopathy [[arxiv](http://arxiv.org/abs/2308.14212)]
  - Domain generalization for diabetic retinopathy 用领域泛化进行糖尿病视网膜病

- Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices [[arxiv](http://arxiv.org/abs/2308.13894)]
  - Federated fine-tuning for large models 大模型联邦微调

- Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis [[arxiv](http://arxiv.org/abs/2308.12495)]
  - Source-free domain adaptation for MRI analysis

- Towards Realistic Unsupervised Fine-tuning with CLIP [[arxiv](http://arxiv.org/abs/2308.12919)]
  - Unsupervised fine-tuning of CLIP

- Fine-tuning can cripple your foundation model; preserving features may be the solution [[arxiv](http://arxiv.org/abs/2308.13320)]
  - Fine-tuning will cripple foundation model

- Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models [[arxiv](http://arxiv.org/abs/2308.07706)]
  - Transfer learning for medical image segmentation

- Transfer Learning for Portfolio Optimization [[arxiv](http://arxiv.org/abs/2307.13546)]
  - Transfer learning for portfolio optimization

- NormAUG: Normalization-guided Augmentation for Domain Generalization [[arxiv](http://arxiv.org/abs/2307.13492)]
  - Normalization augmentation for domain generalization

## 2023-07

- Benchmarking Algorithms for Federated Domain Generalization [[arxiv](http://arxiv.org/abs/2307.04942)]
  - Benchmark algorthms for federated domain generalization 对联邦域泛化算法进行的benchmark

- DISPEL: Domain Generalization via Domain-Specific Liberating [[arxiv](http://arxiv.org/abs/2307.07181)]
  - Domain generalization via domain-specific liberating

- Review of Large Vision Models and Visual Prompt Engineering [[arxiv](https://arxiv.org/abs/2307.00855)]
  - A survey of large vision model and prompt tuning 一个关于大视觉模型的prompt tuning的综述

- Intra- & Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization [[arxiv](https://arxiv.org/abs/2307.00648)]
  - Exemplar-based style synthesis for domain generalization 样例格式合成用于DG

- SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation [[arxiv](https://arxiv.org/abs/2307.01024)]
  - Using SAM for domain adaptation 使用segment anything进行domain adaptation

- Unified Transfer Learning Models for High-Dimensional Linear Regression [[arxiv](https://arxiv.org/abs/2307.00238)]
  - Transfer learning for high-dimensional linar regression 迁移学习用于高维线性回归

## 2023-06

- Pruning for Better Domain Generalizability [[arxiv](http://arxiv.org/abs/2306.13237)]
  - Using pruning for better domain generalization 使用剪枝操作进行domain generalization

- TMLR'23 Generalizability of Adversarial Robustness Under Distribution Shifts [[openreview](https://openreview.net/forum?id=XNFo3dQiCJ)]
  - Evaluate the OOD perormance of adversarial training 评测对抗训练模型的OOD鲁棒性

- Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning [[arxiv](http://arxiv.org/abs/2303.15647)]
  - A guide for parameter-efficient fine-tuning 一个对parameter efficient fine-tuning的全面介绍

- ICML'23 A Kernel-Based View of Language Model Fine-Tuning [[arxiv](http://arxiv.org/abs/2210.05643)]
  - A kernel-based view of language model fine-tuning 一种以kernel的视角来看待fine-tuning的方法

- ICML'23 Improving Visual Prompt Tuning for Self-supervised Vision Transformers [[arxiv](http://arxiv.org/abs/2306.05067)]
  - Improving visual prompt tuning for self-supervision 为自监督模型提高其 prompt tuning 表现

- Cross-Database and Cross-Channel ECG Arrhythmia Heartbeat Classification Based on Unsupervised Domain Adaptation [[arxiv](http://arxiv.org/abs/2306.04433)]
  - EEG using unsupervised domain adaptation 用无监督DA来进行EEG心跳分类

- Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification [[arxiv](http://arxiv.org/abs/2306.03993)]
  - Real-time online unsupervised domain adaptation for REID 无监督DA用于REID

- Federated Domain Generalization: A Survey [[arxiv](http://arxiv.org/abs/2306.01334)]
  - A survey on federated domain generalization 一篇关于联邦域泛化的综述

- Domain Generalization for Domain-Linked Classes [[arxiv](http://arxiv.org/abs/2306.00879)]
  - Domain generalization for domain-linked classes

- Can We Evaluate Domain Adaptation Models Without Target-Domain Labels? A Metric for Unsupervised Evaluation of Domain Adaptation [[arxiv](http://arxiv.org/abs/2305.18712)]
  - Evaluate domain adaptation models 评测domain adaptation的模型

- Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction [[arxiv](http://arxiv.org/abs/2306.00650)]
  - Universal test-time adaptation

- Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting [[arxiv](http://arxiv.org/abs/2306.00409)]
  - Using dynamic visual prompting for model adaptation 用动态视觉prompt进行模型适配

## 2023-05

- Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup [[arxiv](https://arxiv.org/abs/2305.16817)]
  - Why mixup works for domain generalization? 系统性研究为啥mixup对OOD很work

- ACL'23 Parameter-Efficient Fine-Tuning without Introducing New Latency [[arxiv](http://arxiv.org/abs/2305.16742)]
  - Parameter-efficient finetuning 参数高效的finetune

- Universal Domain Adaptation from Foundation Models [[arxiv](http://arxiv.org/abs/2305.11092)]
  - Using foundation models for universal domain adaptation

- Ahead-of-Time P-Tuning [[arxiv](http://arxiv.org/abs/2305.10835)]
  - Ahead-ot-time P-tuning for language models

- Multi-Source to Multi-Target Decentralized Federated Domain Adaptation [[arxiv](http://arxiv.org/abs/2304.12422)]
  - Decentralized federated domain adaptation

- Benchmarking Low-Shot Robustness to Natural Distribution Shifts [[arxiv](http://arxiv.org/abs/2304.11263)]
  - Low-shot robustness to distribution shifts

## 2023-04

- Multi-Source to Multi-Target Decentralized Federated Domain Adaptation [[arxiv](https://arxiv.org/abs/2304.12422)]
  - Multi-source to multi-target federated domain adaptation 多源多目标的联邦域自适应

- ICML'23 AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation [[arxiv](https://arxiv.org/abs/2304.12566)]
  - Adaptive test-time adaptation 非参数化分类器进行测试时adaptation

- Improved Test-Time Adaptation for Domain Generalization [[arxiv](http://arxiv.org/abs/2304.04494)]
  - Improved test-time adaptation for domain generalization

- Reweighted Mixup for Subpopulation Shift [[arxiv](http://arxiv.org/abs/2304.04148)]
  - Reweighted mixup for subpopulation shift

- CVPR'23 Zero-shot Generative Model Adaptation via Image-specific Prompt Learning [[arxiv](http://arxiv.org/abs/2304.03119)]
  - Zero-shot generative model adaptation via image-specific prompt learning 零样本的生成模型adaptation

- Source-free Domain Adaptation Requires Penalized Diversity [[arxiv](http://arxiv.org/abs/2304.02798)]
  - Source-free DA requires penalized diversity

- Domain Generalization with Adversarial Intensity Attack for Medical Image Segmentation [[arxiv](http://arxiv.org/abs/2304.02720)]
  - Domain generalization for medical segmentation 用domain generalization进行医学分割

- CVPR'23 Meta-causal Learning for Single Domain Generalization [[arxiv](http://arxiv.org/abs/2304.03709)]
  - Meta-causal learning for domain generalization

- Domain Generalization In Robust Invariant Representation [[arxiv](http://arxiv.org/abs/2304.03431)]
  - Domain generalization in robust invariant representation

- Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness [[arxiv](http://arxiv.org/abs/2303.16861)]
  - Local structure preserving for adversarial robustness 通过保留局部结构来进行对抗鲁棒性

- TFS-ViT: Token-Level Feature Stylization for Domain Generalization [[arxiv](http://arxiv.org/abs/2303.15698)]
  - Token-level feature stylization for domain generalization 用token-level特征变换进行domain generalization

- Are Data-driven Explanations Robust against Out-of-distribution Data? [[arxiv](http://arxiv.org/abs/2303.16390)]
  - Data-driven explanations robust? 探索数据驱动的解释是否是OOD鲁棒的

- ERM++: An Improved Baseline for Domain Generalization [[arxiv](http://arxiv.org/abs/2304.01973)]
  - Improved ERM for domain generalization 提高的ERM用于domain generalization

- CVPR'23 Feature Alignment and Uniformity for Test Time Adaptation [[arxiv](http://arxiv.org/abs/2303.10902)]
  - Feature alignment for test-time adaptation 使用特征对齐进行测试时adaptation

- Finding Competence Regions in Domain Generalization [[arxiv](http://arxiv.org/abs/2303.09989)]
  - Finding competence regions in domain generalization 在DG中发现能力区域

- CVPR'23 TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization [[arxiv](http://arxiv.org/abs/2303.11135)]
  - Improve generalization and adversarial robustness 同时提高鲁棒性和泛化性

- CVPR'23 Trainable Projected Gradient Method for Robust Fine-tuning [[arxiv](http://arxiv.org/abs/2303.10720)]
  - Trainable PGD for robust fine-tuning 可训练的pgd用于鲁棒的微调技术

- Parameter-Efficient Tuning Makes a Good Classification Head [[arxiv](http://arxiv.org/abs/2210.16771)]
  - Parameter-efficient tuning makes a good classification head 参数高效的迁移学习成就一个好的分类头

- Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning [[arxiv](http://arxiv.org/abs/2303.15833)]
  - Continual domain shift learning using adaptation and generalization 使用 adaptation和DG进行持续分布变化的学习

## 2023-03

- CVPR'23 A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation [[arxiv](http://arxiv.org/abs/2303.09165)]
  - A new benchmark for domain adaptation 一个对于domain adaptation最新的benchmark

- Unsupervised domain adaptation by learning using privileged information [[arxiv](http://arxiv.org/abs/2303.09350)]
  - Domain adaptation by privileged information 使用高级信息进行domain adaptation

- A Unified Continual Learning Framework with General Parameter-Efficient Tuning [[arxiv](http://arxiv.org/abs/2303.10070)]
  - A continual learning framework for parameter-efficient tuning 一个对于参数高效迁移的连续学习框架

- CVPR'23 Sharpness-Aware Gradient Matching for Domain Generalization [[arxiv](http://arxiv.org/abs/2303.10353)]
  - Sharpness-aware gradient matching for DG 利用梯度匹配进行domain generalization

- TempT: Temporal consistency for Test-time adaptation [[arxiv](http://arxiv.org/abs/2303.10536)]
  - Temporeal consistency for test-time adaptation 时间一致性用于test-time adaptation

- TMLR'23 Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks [[arxiv](http://arxiv.org/abs/2303.10455)]
  - A framework for online learning 一个在线学习的框架

- ICLR'23 workshop SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models [[arxiv](http://arxiv.org/abs/2303.10464)]
  - Sparse pre-training and dense fine-tuning

- CVPR'23 ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization [[arxiv](http://arxiv.org/abs/2303.11674)]
  - A lightweight module for domain generalization 一个用于DG的轻量级模块

- ICLR'23 Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning [[arxiv](http://arxiv.org/abs/2303.11866)]
  - Contrastive alignment for vision language models using transfer learning 使用参数高效迁移进行视觉语言模型的对比对齐

- Probabilistic Domain Adaptation for Biomedical Image Segmentation [[arxiv](http://arxiv.org/abs/2303.11790)]
  - Probabilistic domain adaptation for biomedical image segmentation 概率的domain adaptation用于生物医疗图像分割

- Imbalanced Domain Generalization for Robust Single Cell Classification in Hematological Cytomorphology [[arxiv](https://arxiv.org/abs/2303.07771)]
  - Imbalanced domain generalization for single cell classification 不平衡的DG用于单细胞分类

- Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm [[arxiv](https://arxiv.org/abs/2303.07910)]
  - Parameter-efficient transfer learning: a two-stage approach 一种两阶段的参数高效迁移学习

- Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow [[arxiv](https://arxiv.org/abs/2303.07564)]
  - Domain adaptation for foggy scene optical flow 领域自适应用于雾场景的光流

- ICLR'23 AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph Neural Networks [[arxiv](https://arxiv.org/abs/2303.07669)]
  - GNN with autoML transfer learning 用于GNN的自动迁移学习

- Transfer Learning for Real-time Deployment of a Screening Tool for Depression Detection Using Actigraphy [[arxiv](https://arxiv.org/abs/2303.07847)]
  - Transfer learning for Depression detection 迁移学习用于脉动计焦虑检测

- Domain Generalization via Nuclear Norm Regularization [[arxiv](https://arxiv.org/abs/2303.07527)]
  - Domain generalization via nuclear norm regularization 使用核归一化进行domain generalization

- To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning [[arxiv](https://arxiv.org/abs/2303.03374)]
  - Ensembling in transfer learning 调研迁移学习中的集成

- CVPR'13 Masked Images Are Counterfactual Samples for Robust Fine-tuning [[arxiv](https://arxiv.org/abs/2303.03052)]
  - Masked images for robust fine-tuning 调研masked image对于fine-tuning的影响

- FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning [[arxiv](https://arxiv.org/abs/2302.13485v1)]
  - Fast generalization for federated CLIP 在联邦中进行快速的CLIP训练

- Robust Representation Learning with Self-Distillation for Domain Generalization [[arxiv](http://arxiv.org/abs/2302.06874)]
  - Robust representation learning with self-distillation

- ICLR-23 Temporal Coherent Test-Time Optimization for Robust Video Classification [[arxiv](http://arxiv.org/abs/2302.14309)]
  - Temporal distribution shift in video classification

- WSDM-23 A tutorial on domain generalization [[link](https://dl.acm.org/doi/10.1145/3539597.3572722)] | [[website](https://dgresearch.github.io/)]
  - A tutorial on domain generalization

## 2023-02

- On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective [[arxiv](https://arxiv.org/abs/2302.12095)] | [[code](https://github.com/microsoft/robustlearn)]
  - Adversarial and OOD evaluation of ChatGPT 对ChatGPT鲁棒性的评测

- Transfer learning for process design with reinforcement learning [[arxiv](https://arxiv.org/abs/2302.03375)]
  - Transfer learning for process design with reinforcement learning 使用强化迁移学习进行过程设计

- Domain Adaptation for Time Series Under Feature and Label Shifts [[arxiv](https://arxiv.org/abs/2302.03133)]
  - Domain adaptation for time series 用于时间序列的domain adaptation

- How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts? [[arxiv](https://arxiv.org/abs/2302.03679)]
  - Regression models uncertainty for distribution shift 回归模型对于分布漂移的不确定性

- ICLR'23 SoftMatch: Addressing the Quantity-Quality Tradeoff in Semi-supervised Learning [[arxiv](https://arxiv.org/abs/2301.10921)]
  - Semi-supervised learning algorithm 解决标签质量问题的半监督学习方法

- Empirical Study on Optimizer Selection for Out-of-Distribution Generalization [[arxiv](http://arxiv.org/abs/2211.08583)]
  - Opimizer selection for OOD generalization OOD泛化中的学习器选择

- ICML'22 Understanding the failure modes of out-of-distribution generalization [[arxiv](https://openreview.net/forum?id=fSTD6NFIW_b)]
  - Understand the failure modes of OOD generalization 探索OOD泛化中的失败现象

- ICLR'23 Out-of-distribution Representation Learning for Time Series Classification [[arxiv](https://arxiv.org/abs/2209.07027)]
  - OOD for time series classification 时间序列分类的OOD算法

## 2023-01

- ICLR'23 FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning [[arxiv](https://arxiv.org/abs/2205.07246)]
  - New baseline for semi-supervised learning 半监督学习新算法

- CLIP the Gap: A Single Domain Generalization Approach for Object Detection [[arxiv](https://arxiv.org/abs/2301.05499)]
  - Using CLIP for domain generalization object detection 使用CLIP进行域泛化的目标检测

- Language-Informed Transfer Learning for Embodied Household Activities [[arxiv](https://arxiv.org/abs/2301.05318)]
  - Transfer learning for robust control in household 在家居机器人上使用强化迁移学习

- Does progress on ImageNet transfer to real-world datasets? [[arxiv](https://arxiv.org/abs/2301.04644)]
  - ImageNet accuracy does not transfer to down-stream tasks

- TPAMI'23 Source-Free Unsupervised Domain Adaptation: A Survey [[arxiv](http://arxiv.org/abs/2301.00265)]
  - A survey on source-free domain adaptation 关于source-free DA的一个最新综述

- Discriminative Radial Domain Adaptation [[arxiv](http://arxiv.org/abs/2301.00383)]
  - Discriminative radial domain adaptation 判别性的放射式domain adaptation

## 2022-12

- WACV'23 Cross-Domain Video Anomaly Detection without Target Domain Adaptation [[arxiv](https://arxiv.org/abs/2212.07010)]
  - Cross-domain video anomaly detection without target domain adaptation 跨域视频异常检测

- Co-Learning with Pre-Trained Networks Improves Source-Free Domain Adaptation [[arxiv](https://arxiv.org/abs/2212.07585)]
  - Pre-trained models for source-free domain adaptation 用预训练模型进行source-free DA

- TMLR'22 A Unified Survey on Anomaly, Novelty, Open-Set, and Out of-Distribution Detection: Solutions and Future Challenges [[openreview](https://openreview.net/forum?id=aRtjVZvbpK)]
  - A recent survey on OOD/anomaly detection 一篇最新的关于OOD/anomaly detection的综述

- NeurIPS'18 A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks [[paper](https://proceedings.neurips.cc/paper/2018/hash/abdeb6f575ac5c6676b747bca8d09cc2-Abstract.html)]
  - Using class-conditional distribution for OOD detection 使用类条件概率进行OOD检测

- ICLR'22 Discrete Representations Strengthen Vision Transformer Robustness [[arxiv](http://arxiv.org/abs/2111.10493)]
  - Embed discrete representation for OOD generalization 在ViT中加入离散表征增强OOD性能

- CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars [[arxiv](https://arxiv.org/abs/2212.00621)]
  - Continual DA for self-driving cars 连续的domain adaptation用于自动驾驶

- Finetune like you pretrain: Improved finetuning of zero-shot vision models [[arxiv]](http://arxiv.org/abs/2212.00638)]
  - Improved fine-tuning of zero-shot models 针对zero-shot model提高fine-tuneing

## 2022-11

- Cross-Domain Ensemble Distillation for Domain Generalization [[arXiv](https://arxiv.org/pdf/2211.14058)] [[Code](https://github.com/leekyungmoon/XDED)]
  - Cross-domain ensemble distillation for domain generalization for domain generalization

- ECCV-22 DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation [[arXiv](https://arxiv.org/pdf/2207.09988.pdf)] [[Code](https://github.com/dvlab-research/DecoupleNet)]
  - Domain adaptation in semantic segmentation 语义分割域适应

- Robust Mean Teacher for Continual and Gradual Test-Time Adaptation [[arxiv](https://arxiv.org/abs/2211.13081)]
  - Mean teacher for test-time adaptation 在测试时用mean teacher进行适配

- Learning to Learn Domain-invariant Parameters for Domain Generalization [[arxiv](Learning to Learn Domain-invariant Parameters for Domain Generalization)]
  - Learning to learn domain-invariant parameters for DG 元学习进行domain generalization

- HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization [[arxiv](https://arxiv.org/abs/2211.08253)]
  - Hypernetwork-based ensembling for domain generalization 超网络构成的集成学习用于domain generalization

- The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning [[arxiv](https://arxiv.org/abs/2106.15831)]
  - OOD using fine-tuning 系统总结了基于fine-tuning进行OOD的一些结果

- GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective [[arxiv](https://arxiv.org/abs/2211.08073)]
  - OOD for natural language processing evaluation 提出GLUE-X用于OOD在NLP数据上的评估

- CVPR'22 Delving Deep Into the Generalization of Vision Transformers Under Distribution Shifts [[arxiv](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Delving_Deep_Into_the_Generalization_of_Vision_Transformers_Under_Distribution_CVPR_2022_paper.html)]
  - Vision transformers generalization under distribution shifts 评估ViT的分布漂移

- NeurIPS'22 Models Out of Line: A Fourier Lens on Distribution Shift Robustness [[arxiv](https://openreview.net/forum?id=YZ-N-sejjwO)]
  - A fourier lens on distribution shift robustness 通过傅里叶视角来看分布漂移的鲁棒性

- CVPR'22 Does Robustness on ImageNet Transfer to Downstream Tasks? [[arxiv](https://openaccess.thecvf.com/content/CVPR2022/papers/Yamada_Does_Robustness_on_ImageNet_Transfer_to_Downstream_Tasks_CVPR_2022_paper.pdf)]
  - Does robustness on imagenet transfer lto downstream tasks?

- Normalization Perturbation: A Simple Domain Generalization Method for Real-World Domain Shifts [[arxiv](https://arxiv.org/abs/2211.04393)]
  - Normalization perturbation for domain generalization 通过归一化扰动来进行domain generalization

- FIXED: Frustraitingly easy domain generalization using Mixup [[arxiv](https://arxiv.org/pdf/2211.05228.pdf)]
  - 使用Mixup进行domain generalization

- Learning to Learn Domain-invariant Parameters for Domain Generalization [[arxiv](https://arxiv.org/abs/2211.04582)]
  - Learning to learn domain-invariant parameters for domain generalization

- NeurIPS'22 Improved Fine-Tuning by Better Leveraging Pre-Training Data [[openreview](https://openreview.net/forum?id=YTXIIc7cAQ)]
  - Using pre-training data for fine-tuning 用预训练数据来做微调

- NeurIPS'22 Divide and Contrast: Source-free Domain Adaptation via Adaptive Contrastive Learning [[openreview](https://openreview.net/forum?id=NjImFaBEHl)]
  - Adaptive contrastive learning for source-free DA 自适应的对比学习用于source-free DA

- NeurIPS'22 LOG: Active Model Adaptation for Label-Efficient OOD Generalization [[openreview](https://openreview.net/forum?id=VdQWVdT_8v)]
  - Model adaptation for label-efficient OOD generalization

- NeurIPS'22 MetaTeacher: Coordinating Multi-Model Domain Adaptation for Medical Image Classification [[openreview](https://openreview.net/forum?id=AQd4ugzALQ1)]
  - Multi-model domain adaptation mor medical image classification 多模型DA用于医疗数据

- NeurIPS'22 Domain Adaptation under Open Set Label Shift [[openreview](https://openreview.net/forum?id=OMZG4vsKmm7)]
  - Domain adaptation under open set label shift 在开放集的label shift中的DA

- NeurIPS'22 Domain Generalization without Excess Empirical Risk [[openreview](https://openreview.net/forum?id=pluyPFTiTeJ)]
  - Domain generalization without excess empirical risk

- NeurIPS'22 FedSR: A Simple and Effective Domain Generalization Method for Federated Learning [[openreview](https://openreview.net/forum?id=mrt90D00aQX)]
  - FedSR for federated learning domain generalization 用于联邦学习的domain generalization

- NeurIPS'22 Probable Domain Generalization via Quantile Risk Minimization [[openreview](https://openreview.net/forum?id=6FkSHynJr1)]
  - Domain generalization with quantile risk minimization 用quantile风险最小化的domain generalization

- NeurIPS'22 Beyond Not-Forgetting: Continual Learning with Backward Knowledge Transfer [[arxiv](http://arxiv.org/abs/2211.00789)]
  - Continual learning with backward knowledge transfer 反向知识迁移的持续学习

- NeurIPS'22 Test Time Adaptation via Conjugate Pseudo-labels [[openreview](https://openreview.net/forum?id=2yvUYc-YNUH)]
  - Test-time adaptation with conjugate pseudo-labels 用伪标签进行测试时adaptation

- NeurIPS'22 Your Out-of-Distribution Detection Method is Not Robust! [[openreview](https://openreview.net/forum?id=YUEP3ZmkL1)]
  - OOD models are not robust 分布外泛化模型不够鲁棒


## 2022-10

- NeurIPS'22 Respecting Transfer Gap in Knowledge Distillation [[arxiv](http://arxiv.org/abs/2210.12787)]
  - Transfer gap in distillation 知识蒸馏中的迁移gap

- Transfer of Machine Learning Fairness across Domains [[arxiv](http://arxiv.org/abs/1906.09688)]
  - Fairness transfer in transfer learning 迁移学习中的公平性迁移

- On Fine-Tuned Deep Features for Unsupervised Domain Adaptation [[arxiv](http://arxiv.org/abs/2210.14083)]
  - Fine-tuned features for domain adaptation 微调的特征用于域自适应

- WACV-23 ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing [[arxiv](https://arxiv.org/abs/2210.11539)]
  - Domain adaptation for object detection using confidence mixing 用置信度mix做domain adaptation

- CVPR-20 Regularizing CNN Transfer Learning With Randomised Regression [[arxiv](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhong_Regularizing_CNN_Transfer_Learning_With_Randomised_Regression_CVPR_2020_paper.html)]
  - Using randomized regression to regularize CNN 用随机回归约束CNN迁移学习

- AAAI-21 TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning [[arxiv](https://ojs.aaai.org/index.php/AAAI/article/view/17046)]
  - Pruning pre-trained model for transfer learning 通过对预训练模型进行剪枝来进行迁移学习

- PhDthesis Generalizing in the Real World with Representation Learning [[arxiv](http://arxiv.org/abs/2210.09925)]
  - A phd thesis about generalization in real world 一篇关于现实世界如何做Generalization的博士论文

- The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning [[arxiv](https://openreview.net/forum?id=Qs3EfpieOh)]
  - Evolution of OOD robustness by fine-tuning

- Visual Prompt Tuning for Test-time Domain Adaptation [[arxiv](http://arxiv.org/abs/2210.04831)]
  - VPT for test-time adaptation 用prompt tuning进行test-time DA

- Unsupervised Domain Adaptation for COVID-19 Information Service with Contrastive Adversarial Domain Mixup [[arxiv](https://arxiv.org/abs/2210.03250)]
  - Domain adaptation for COVID-19 用DA进行COVID-19预测

- ICONIP'22 IDPL: Intra-subdomain adaptation adversarial learning segmentation method based on Dynamic Pseudo Labels [[arxiv](https://arxiv.org/abs/2210.03435)]
  - Intra-domain adaptation for segmentation 子领域对抗Adaptation

- NeurIPS'22 Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks [[arxiv](https://arxiv.org/abs/2210.03265)]
  - Parameter-efficient multi-task adaptation 参数高效的多任务adaptation

- Out-of-Distribution Generalization in Algorithmic Reasoning Through Curriculum Learning [[arxiv](https://arxiv.org/abs/2210.03275)]
  - OOD in algorithmic reasoning 算法reasoning过程中的OOD

- Towards Out-of-Distribution Adversarial Robustness [[arxiv](https://arxiv.org/abs/2210.03150)]
  - OOD adversarial robustness OOD对抗鲁棒性

- TripleE: Easy Domain Generalization via Episodic Replay [[arxiv](https://arxiv.org/pdf/2210.01807.pdf)]
  - Easy domain generalization by episodic replay

- Deep Spatial Domain Generalization [[arxiv](https://web7.arxiv.org/pdf/2210.00729.pdf)]
  - Deep spatial domain generalization

## 2022-09

- Assaying Out-Of-Distribution Generalization in Transfer Learning [[arXiv](http://arxiv.org/abs/2207.09239)]
  - A lot of experiments to show OOD performance

- ICML-21 Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization [[arxiv](https://proceedings.mlr.press/v139/miller21b.html)]
  - Strong correlation between ID and OOD

- Deep Domain Adaptation for Detecting Bomb Craters in Aerial Images [[arxiv](https://arxiv.org/abs/2209.11299)]
  - Bomb craters detection using domain adaptation 用DA检测遥感图像中的炮弹弹坑

- WACV-23 TeST: Test-time Self-Training under Distribution Shift [[arxiv](https://arxiv.org/abs/2209.11459)]
  - Test-time self-training 测试时训练

- StyleTime: Style Transfer for Synthetic Time Series Generation [[arxiv](https://arxiv.org/abs/2209.11306)]
  - Style transfer for time series generation 时间序列生成的风格迁移

- Robust Domain Adaptation for Machine Reading Comprehension [[arxiv](https://arxiv.org/abs/2209.11615)]
  - Domain adaptation for machine reading comprehension 机器阅读理解的domain adaptation

- Generalized representations learning for time series classification [[arxiv](https://arxiv.org/abs/2209.07027)]
  - OOD for time series classification 域泛化用于时间序列分类

- USB: A Unified Semi-supervised Learning Benchmark [[arxiv](https://arxiv.org/abs/2208.07204)] [[code](https://github.com/microsoft/Semi-supervised-learning)]
  - Unified semi-supervised learning codebase 半监督学习统一代码库

- Test-Time Training with Masked Autoencoders [[arxiv](https://arxiv.org/abs/2209.07522)]
  - Test-time training with MAE MAE的测试时训练

- Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models [[arxiv](https://arxiv.org/abs/2209.07511)]
  - Test-time prompt tuning 测试时的prompt tuning

- TeST: test-time self-training under distribution shift [[arxiv](https://assets.amazon.science/02/1c/b469914c4732a9c29ac765f948f9/test-test-time-self-training-under-distribution-shift.pdf)]
  - Test-time self-training 测试时的self-training

- Language-aware Domain Generalization Network for Cross-Scene Hyperspectral Image Classification [[arxiv](https://arxiv.org/pdf/2209.02700.pdf)]
  - Domain generalization for cross-scene hyperspectral image classification 域泛化用于高光谱图像分类

- IEEE-TMM'22 Uncertainty Modeling for Robust Domain Adaptation Under Noisy Environments [[IEEE](https://ieeexplore.ieee.org/abstract/document/9882310)]
  - Uncertainty modeling for domain adaptation 噪声环境下的domain adaptation

- Improving Robustness to Out-of-Distribution Data by Frequency-based Augmentation [arxiv](https://arxiv.org/abs/2209.02369)
  - OOD by frequency-based augmentation 通过基于频率的数据增强进行OOD

- Domain Generalization for Prostate Segmentation in Transrectal Ultrasound Images: A Multi-center Study [arxiv](https://arxiv.org/abs/2209.02126)
  - Domain generalizationfor prostate segmentation 领域泛化用于前列腺分割

- Domain Adaptation from Scratch [arxiv](https://arxiv.org/abs/2209.00830)
  - Domain adaptation from scratch

- Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution [arxiv](https://arxiv.org/abs/2209.00652)
  - Model selection for domain generalization 域泛化中的模型选择问题

- [Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets](https://arxiv.org/pdf/2208.07463.pdf)
  - Parameter efficient CNN adapter for transfer learning 参数高效的CNN adapter用于迁移学习

- [Equivariant Disentangled Transformation for Domain Generalization under Combination Shift](https://arxiv.org/abs/2208.02011)
  - Equivariant disentangled transformation for domain generalization 新的建模domain generalization的思路

## 2022-08

- ECCV-22 workshop [Domain-Specific Risk Minimization](https://arxiv.org/abs/2208.08661)
  - Domain-specific risk minization for OOD 领域特异性风险最小化用于域泛化

- TPAMI [Semi-Supervised and Unsupervised Deep Visual Learning: A Survey](https://arxiv.org/abs/2208.11296)
  - Survey on semi and unsupervsed learning 半监督和无监督综述

- [Improving video retrieval using multilingual knowledge transfer](https://arxiv.org/abs/2208.11553)
  - Video retrieval using multilingual knowledge transfer 多语言知识迁移用于视频检索

- [Transfer Learning-based State of Health Estimation for Lithium-ion Battery with Cycle Synchronization](https://arxiv.org/abs/2208.11204)
  - Battery health estimation using transfer learning 用迁移学习进行电池健康估计

- IJCAI-22 [Domain Generalization through the Lens of Angular Invariance](https://www.ijcai.org/proceedings/2022/0139.pdf)
  - Using angular invariance for domain generalization 使用角度不变性进行domain generalization

- MM-22 [Making the Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2208.01195)
  - Transformer for domain adaptation 用transformer进行DA

- [Adaptive Domain Generalization via Online Disagreement Minimization](https://arxiv.org/abs/2208.01996)
  - Online domain generalization via disagreement minimization 在线DG

- [Self-Distilled Vision Transformer for Domain Generalization](http://arxiv.org/abs/2207.12392)
  - Vision transformer for domain generalization 用ViT做domain generalization

- NeurIPS-21 [The balancing principle for parameter choice in distance-regularized domain adaptation](https://papers.nips.cc/paper/2021/hash/ae0909a324fb2530e205e52d40266418-Abstract.html)
  - Hyperparameter selection for domain adaptation 对adaptation中的正则项系数进行选择

- [Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder](https://arxiv.org/abs/2207.14508)
  - Transfer learning for segmentation problems 统一表示迁移学习于分割问题的思路

## 2022-07

- TMLR-22 [Domain-invariant Feature Exploration for Domain Generalization](https://arxiv.org/abs/2207.12020)
  - Exploring domain-invariant feature for domain generalization 探索领域不变特征在领域泛化中的应用

- TIST-22 [Domain Generalization for Activity Recognition via Adaptive Feature Fusion](https://arxiv.org/abs/2207.11221)
  - Domain generalization for activity recognition 领域泛化用于行为识别

- ECCV-22 [Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2207.10856)
  - Prototype continual domain adaptation 基于原型的类增量domain adaptation

- [Federated Semi-Supervised Domain Adaptation via Knowledge Transfer](https://arxiv.org/abs/2207.10727)
  - Federated semi-supervised DA 联邦半监督DA

- [Hyper-Representations for Pre-Training and Transfer Learning](https://arxiv.org/abs/2207.10951)
  - Hyper-representation for pre-training and fine-tuning 对于预训练和微调的超表示

- MM-22 [Source-Free Domain Adaptation for Real-world Image Dehazing](https://arxiv.org/abs/2207.06644)
  - Source-free DA for image dehazing 无需源域的迁移用于图像去雾

- [Improved OOD Generalization via Conditional Invariant Regularizer](https://arxiv.org/abs/2207.06687)
  - Improved OOD generalization via conditional invariant regularizer 通过条件不变正则进行OOD泛化

- CVPR-22 [Segmenting Across Places: The Need for Fair Transfer Learning With Satellite Imagery](https://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Zhang_Segmenting_Across_Places_The_Need_for_Fair_Transfer_Learning_With_CVPRW_2022_paper.html)
  - Fair transfer learning with satellite imagery 公平迁移学习

- [Transferability-Guided Cross-Domain Cross-Task Transfer Learning](https://arxiv.org/abs/2207.05510)
  - Cross-domain cross-task transfer learning 用迁移性指标指导跨领域跨任务迁移

- [Cross-Architecture Knowledge Distillation](https://arxiv.org/abs/2207.05273)
  - Cross-architecture knowledge distillation 跨架构的知识蒸馏

- ECCV-22 [Knowledge Condensation Distillation](https://arxiv.org/abs/2207.05409)
  - Knowledge condensation distillation 知识压缩蒸馏

- [An Information-Theoretic Analysis for Transfer Learning: Error Bounds and Applications](https://arxiv.org/abs/2207.05377)
  - Information-theoretic analysis for transfer learning 用信息理论解释迁移学习

- [A Data-Based Perspective on Transfer Learning](https://arxiv.org/abs/2207.05739)
  - Analyze the data numbers in transfer learning 分析迁移学习中数据的重要性

- [PAC-Bayesian Domain Adaptation Bounds for Multiclass Learners](https://arxiv.org/abs/2207.05685)
  - PAC-Bayesian domain adaptation 基于PAC-Bayesian的domain adaptation

## 2022-06

- NeurIPS-21 [Parameterized Knowledge Transfer for Personalized Federated Learning](https://proceedings.neurips.cc/paper/2021/hash/5383c7318a3158b9bc261d0b6996f7c2-Abstract.html)
  - personalized group knowledge transfer training
  - 个性化群体知识迁移
- ICML-21 [Federated Continual Learning with Weighted Inter-client Transfer](https://proceedings.mlr.press/v139/yoon21b.html)
  - Federated Weighted Inter-client Transfer (FedWeIT) for Federated Continual Learning
  - 联邦加权客户端间传输方法，用于联邦持续学习
- SIGIR-21 [FedCT: Federated Collaborative Transfer for Recommendation](https://doi.org/10.1145/3404835.3462825)
  - Federated learning for cross-domain recommendation
  - 使用联邦迁移学习执行跨域推荐任务
- KDD-21 [Federated Adversarial Debiasing for Fair and Transferable Representations](https://doi.org/10.1145/3447548.3467281)
  - Federated Adversarial DEbiasing (FADE)
  - 通过对抗性学习对联邦学习过程去除偏见
- NeurIPS-20 [Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge](https://proceedings.neurips.cc/paper/2020/hash/a1d4c20b182ad7137ab3606f0e3fc8a4-Abstract.html)
  - Group knowledge transfer training
  - 群体知识迁移

- FL-IJCAI-22 [MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare](https://arxiv.org/abs/2206.08516)
  - MetaFed: a new form of federated learning 联邦之联邦学习、新范式

- Interspeech-22 [Decoupled Federated Learning for ASR with Non-IID Data](https://jd92.wang/assets/files/DecoupleFL-IS22.pdf)
  - Decoupled federated learning for non IID 解耦的联邦架构用于Non-IID语音识别

- [Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive Representation Learning](https://arxiv.org/abs/2206.10137)
  - Few-shot DA for unsupervised constrastive learning 小样本DA用于无监督对比学习

- [The Importance of Background Information for Out of Distribution Generalization](https://arxiv.org/abs/2206.08794)
  - Background information for OOD generalization 背景信息对于OOD泛化的重要性

- [Zero-Shot AutoML with Pretrained Models](https://arxiv.org/abs/2206.08476)
  - 用预训练模型进行零样本的自动机器学习

- [How robust are pre-trained models to distribution shift?](https://arxiv.org/abs/2206.08871)
  - How robust are pre-trained models to distribution shift 评估预训练模型对于distribution shift的鲁棒性

- [FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification](https://arxiv.org/abs/2206.08671)
  - Few-shot transfer learning for image classification 小样本迁移学习用于图像分类

- [COVID-19 Detection using Transfer Learning with Convolutional Neural Network](https://arxiv.org/abs/2206.08557)
  - COVID-19 using transfer learning 用迁移学习进行COVID-19检测

- [Wav2vec-S: Semi-Supervised Pre-Training for Speech Recognition](https://arxiv.org/abs/2110.04484)
  - Pretraining for speech recognition 用预训练模型进行语音识别

- [Causal Balancing for Domain Generalization](https://arxiv.org/abs/2206.05263)
  - Causal balancing for domain generalization 因果平衡用于领域泛化

- NAACL-22 [Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning](https://arxiv.org/abs/2206.03715)
  - Transfer learning for zero-shot reasoning 迁移学习用于零次常识推理

- [ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation](https://arxiv.org/abs/2206.03888)
  - Fewshot UDA for medical image segmentation 小样本域自适应用于医疗图像分割

- [One Ring to Bring Them All: Towards Open-Set Recognition under Domain Shift](https://arxiv.org/abs/2206.03600)
  - Open set recognition with domain shift 开放集+domain shift

- [Toward Certified Robustness Against Real-World Distribution Shifts](https://arxiv.org/abs/2206.03669)
  - Certified robustness against real-world distribution shifts 真实世界中的distribution shift

- [On Transfer Learning in Functional Linear Regression](https://arxiv.org/abs/2206.04277)
  - Transfer learning in functional linear regression 迁移学习用于函数式线性回归

## 2022-05

- IJCAI-22 [Parameter-Efficient Sparsity for Large Language Models Fine-Tuning](https://arxiv.org/abs/2205.11005)
  - Parameter-efficient sparsity for language model fine-tuning 参数高效的稀疏学习用于语言模型微调

- [A Domain-adaptive Pre-training Approach for Language Bias Detection in News](https://arxiv.org/abs/2205.10773)
  - Domain-adaptive pre-training for language bias detection 领域适配预训练用于新闻语言偏见检测

- [ScholarBERT: Bigger is Not Always Better](https://arxiv.org/abs/2205.11342)
  - Empirical study on fine-tuning experiments 提出ScholarBERT进行大规模finetuning实验

- ICPR-22 [OTAdapt: Optimal Transport-based Approach For Unsupervised Domain Adaptation](https://arxiv.org/abs/2205.10738)
  - Optimal transport-based domain adaptation 利用最优传输进行领域自适应

- [Temporal Domain Generalization with Drift-Aware Dynamic Neural Network](https://arxiv.org/abs/2205.10664)
  - Temporal domain generalization with drift-aware dynamic neural network 时序域泛化

- [Active Source Free Domain Adaptation](https://arxiv.org/abs/2205.10711)
  - Active source-free DA 主动学习-无源域DA

- [Test-Time Robust Personalization for Federated Learning](https://arxiv.org/abs/2205.10920)
  - Test-time robust personalization for FL 测试时鲁棒联邦学习

- [FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning](https://arxiv.org/abs/2205.07246)
  - Self-adaptive thresholding for semi-supervised learning 新的自适应阈值半监督方法

- IJCAI-22 [Test-time Fourier Style Calibration for Domain Generalization](https://arxiv.org/abs/2205.06427)
  - Test-time calibration for domain generalization 用傅立叶变化进行域泛化的测试时矫正

- [Multiple Domain Causal Networks](https://arxiv.org/abs/2205.06791)
  - Mlutiple domain causal networks 多领域的因果网络

- ICLR-22 [Enhancing Cross-lingual Transfer by Manifold Mixup](https://arxiv.org/abs/2205.04182)
  - Cross-lingual transfer using manifold mixup 用Mixup进行cross-lingual transfer

- CVPR-22 workshop [Online Unsupervised Domain Adaptation for Person Re-identification](https://arxiv.org/abs/2205.04383)
  - Online domain adaptation for REID 在线adaptation

- [Time-Series Domain Adaptation via Sparse Associative Structure Alignment: Learning Invariance and Variance](https://arxiv.org/abs/2205.03554)
  - Time series domain adaptation 时间序列domain adaptation

- TIP-22 [Spot-adaptive Knowledge Distillation](https://arxiv.org/abs/2205.02399)
  - Spot-adaptive knowledge distillation 层次自适应的知识蒸馏

- NAACL-22 [Efficient Few-Shot Fine-Tuning for Opinion Summarization](https://arxiv.org/abs/2205.02170)
  - Few-shot fine-tuning for opinion summarization 小样本微调技术用于评论总结

- ICME-22 [Unsupervised Domain Adaptation Learning for Hierarchical Infant Pose Recognition with Synthetic Data](https://arxiv.org/abs/2205.01892)
  - Unsupervised domain adaptation for infant pose recognition 用领域自适应进行婴儿姿势识别

## 2022-04

*Updated at 2022-04-29:*

- ACL-22 [Probing Simile Knowledge from Pre-trained Language Models](https://arxiv.org/abs/2204.12807)
  - Probe simile knowledge from pre-trained model 从预训练模型中找出明喻知识

- [Parkinson's disease diagnostics using AI and natural language knowledge transfer](https://arxiv.org/abs/2204.12559)
  - Transfer learning for Parkinson's disease diagnostics 迁移学习用于帕金森诊断

- CVPR-22 [MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2204.12667)
  - Multi-modal test-time adaptation for 3D semantic segmentation 多模态测试时adaptation用于3D语义分割

- [Transfer Learning with Pre-trained Conditional Generative Models](https://arxiv.org/abs/2204.12833)
  - Transfer learning with pre-trained conditional generative models 条件生成模型用于迁移学习

- ICLR-22 [Towards a Unified View of Parameter-Efficient Transfer Learning](https://openreview.net/forum?id=0RDcd5Axok)
  - Unified view of parameter-efficient transfer learning 一个统一视角看待参数高效的迁移学习

- ICLR-22 [Exploring the Limits of Large Scale Pre-training](https://openreview.net/forum?id=V3C8p78sDa)
  - Many experiments to explore pre-training  许多实验来探索预训练

- IEEE TNNLS-22 [Towards Personalized Federated Learning](http://arxiv.org/abs/2103.00710)
  - A survey on personalized federated learning 一个关于个性化联邦学习的综述

- [On Effectively Learning of Knowledge in Continual Pre-training](https://arxiv.org/abs/2204.07994)
  - Continual per-training 持续的预训练

- [Just Fine-tune Twice: Selective Differential Privacy for Large Language Models](https://arxiv.org/abs/2204.07667)
  - Differential privacy by just fine-tune twice 通过微调两次进行差分隐私

- CVPR-22 [Safe Self-Refinement for Transformer-based Domain Adaptation](https://arxiv.org/abs/2204.07683)
  - Transformer-based domain adaptation 基于transformer的domain adaptation

- [Undoing the Damage of Label Shift for Cross-domain Semantic Segmentation](https://arxiv.org/abs/2204.05546)
  - Handle the label shift in cross-domain semantic segmentation  在跨域语义分割时考虑label shift

- CVPR-22 workshop [Out-Of-Distribution Detection In Unsupervised Continual Learning](https://arxiv.org/abs/2204.05462)
  - OOD detection in unsupervised continual learning 无监督持续学习中进行OOD检测

- [Transfer Learning for Autonomous Chatter Detection in Machining](https://arxiv.org/abs/2204.05400)
  - Transfer learning for autonomous chatter detection

- NAACL-22 [GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering](https://arxiv.org/abs/2204.04179)
  - Fast fine-tuning for content-based collaborative filtering
  - 快速的适用于协同过滤的微调

- AAAI-22 [Powering Finetuning in Few-shot Learning: Domain-Agnostic Feature Adaptation with Rectified Class Prototypes](https://arxiv.org/abs/2204.03749)
  - Finetuning in few-shot learning
  - 小样本学习中的微调

- CVPR-22 [Does Robustness on ImageNet Transfer to Downstream Tasks?](https://arxiv.org/abs/2204.03934)
  - Transfer learning robustness
  - 迁移学习鲁棒性

- [Blockchain as an Enabler for Transfer Learning in Smart Environments](https://arxiv.org/abs/2204.03959)
  - Blockchain transfer learning
  - 用区块链进行迁移学习

- ICLR-22 [Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution](https://openreview.net/forum?id=UYneFzXSJWh)
  - Fin-tuning and linear probing for ood generalization
  - 先linear probing最后一层再finetune对OOD任务最好

- ICLR-22 [Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks](https://openreview.net/forum?id=avgclFZ221l)
  - Asymmetry learning for OOD tasks
  - 非对称学习用于OOD任务

## 2022-03

- [Gated Domain-Invariant Feature Disentanglement for Domain Generalizable Object Detection](https://arxiv.org/abs/2203.11432)
  - Channel masking for domain generalization object detection
  - 通过一个gate控制channel masking进行object detection DG

- [A Broad Study of Pre-training for Domain Generalization and Adaptation](https://arxiv.org/abs/2203.11819)
  - A broad study of pre-training models for DA and DG
  - 大量的实验进行DA和DG

- ISPASS-22 [Benchmarking Test-Time Unsupervised Deep Neural Network Adaptation on Edge Devices](https://arxiv.org/abs/2203.11295)
  - Benchmarking test-time adaptation for edge devices
  - 在端设备上评测test-time adaptation算法

- [Multi-Source Domain Adaptation Based on Federated Knowledge Alignment](https://arxiv.org/abs/2203.11635)
  - Multi-source domain adaptation
  - 多源域自适应

- [Improving Generalization in Federated Learning by Seeking Flat Minima](https://arxiv.org/abs/2203.11834)
  - Seeking flat minima for domain generalization in federated learning
  - 通过寻找平坦值进行联邦学习领域泛化

- CVPR-22 [Decoupled Knowledge Distillation](https://arxiv.org/abs/2203.08679)
  - Decoupled knowledge distillation
  - 解耦的知识蒸馏

- [SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence](https://arxiv.org/abs/2203.08176)
  - Personalized federated learning
  - 个性化联邦学习

- ICSE-22 [ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing](https://link.zhihu.com/?target=https%3A//jd92.wang/assets/files/icse22-remos.pdf) | [Code](https://github.com/ziqi-zhang/ReMoS_artifact) | [Blog](https://zhuanlan.zhihu.com/p/446453487) | [Video](https://www.bilibili.com/video/BV1mi4y1C7bP)
  - Safe transfer learning by reducing defect inheritance
  - 安全迁移学习的最新工作

- ACL-22 [Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features](https://arxiv.org/abs/2203.03191)
  - Language-agnostic meta-learning for TTS
  - 语言无关的元学习用于TTS

- [Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models](https://arxiv.org/abs/2203.03131)
  - Adapt unfamiliar inputs to frozen pretrained models
  - 让固定的预训练模型适配不熟悉的输入

- [One Model, Multiple Tasks: Pathways for Natural Language Understanding](https://arxiv.org/abs/2203.03312)
  - Pathways for natural language understanding
  - 使用一个model用于所有NLP任务

- [Pre-trained Token-replaced Detection Model as Few-shot Learner](https://arxiv.org/abs/2203.03235)
  - Pre-trained token-replaced detection model as few-shot learner
  - 预训练的替换token的检测模型

- [Open Set Domain Adaptation By Novel Class Discovery](https://arxiv.org/abs/2203.03329)
  - Open set DA by novel class discovery
  - 基于新类发现的open set da

- ICML-21 workshop [Domain Adaptation with Factorizable Joint Shift](https://arxiv.org/abs/2203.02902)
  - Domain adaptation with factorizable joint shift
  - 基于可分解的联合漂移的领域自适应

- ICC-22 [Knowledge Transfer in Deep Reinforcement Learning for Slice-Aware Mobility Robustness Optimization](https://arxiv.org/abs/2203.03227)
  - Knowledge transfer in RL
  - 强化迁移学习

- ACL-22 [Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings](https://arxiv.org/abs/2203.00211)
  - Investigate selective prediction approaches in IID, OOD, and ADV settings
  - 在独立同分布、分布外、对抗情境中调研选择性预测方法

- PAKDD-22 [Layer Adaptive Deep Neural Networks for Out-of-distribution Detection](https://arxiv.org/abs/2203.00192)
  - Layer adaptive network for OOD detection
  - 层自适应的网络进行OOD检测

- [Learning Semantic Segmentation from Multiple Datasets with Label Shifts](https://arxiv.org/abs/2202.14030)
  - Learning semantic segmentation from many datasets with label shifts
  - 在有标签漂移的情况下从多个数据集中学习语义分割

- [Causal Domain Adaptation with Copula Entropy based Conditional Independence Test](https://arxiv.org/abs/2202.13482)
  - Use copula entropy based conditional independence test for csusal domain adaptation
  - 使用基于copula entopy的条件独立测试进行causal domain adaptation

- [Interpretable Concept-based Prototypical Networks for Few-Shot Learning](https://arxiv.org/abs/2202.13474)
  - Concept-based prototypical network for few-shot learning
  - 基于概念的原型网络用于小样本学习

## 2022-02

- [How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?](https://arxiv.org/abs/2202.09014)
  - Self-supervised learning for cross-domain few-shot
  - 自监督用于跨领域小样本

- [Deep Transfer Learning on Satellite Imagery Improves Air Quality Estimates in Developing Nations](https://arxiv.org/abs/2202.08890)
  - Deep transfer learning for air quality estimate
  - 深度迁移学习用于卫星图到空气质量预测

- ICLR-22 oral [A Fine-Grained Analysis on Distribution Shift](https://openreview.net/forum?id=Dl4LetuLdyK)
  - Extensive experiments on distribution shift for OOD
  - 大量的实验进行OOD验证

- ICLR-22 oral [Fine-Tuning Distorts Pretrained Features and Underperforms Out-of-Distribution](https://openreview.net/forum?id=UYneFzXSJWh)
  - Fine-tuning with linear probing for OOD
  - 微调加上linear probing用于OOD

- ICLR-22 spotlight [Towards a Unified View of Parameter-Efficient Transfer Learning](https://openreview.net/pdf?id=0RDcd5Axok)
  - Detailed analysis of parameter-efficient transfer learning
  - 对参数高效的迁移学习进行分析

- ICLR-22 [Graph-Relational Domain Adaptation](https://arxiv.org/abs/2202.03628)
  - Graph-relational domain adapttion using topological structures
  - 图级别的domain adaptation，使用拓扑结构

- [Domain Adversarial Spatial-Temporal Network: A Transferable Framework for Short-term Traffic Forecasting across Cities](https://arxiv.org/abs/2202.03630)
  - Transfer learning for traffic forecasting across cities
  - 用迁移学习进行跨城市的交通流量预测

- ICLR-22 [Uncertainty Modeling for Out-of-Distribution Generalization](https://arxiv.org/abs/2202.03958)
  - Uncertainty modeling for OOD generalization
  - 用于分布外泛化的不确定性建模

- ICLR-22 [BEiT: BERT Pre-Training of Image Transformers](https://openreview.net/forum?id=p-BhZSz59o4)
  - BERT pre-training of image transformers
  - 用BERT的方式pre-train transformer

- [Improved Fine-tuning by Leveraging Pre-training Data: Theory and Practice](http://arxiv.org/abs/2111.12292)
  - Using pre-training data to improve fine-tuning
  - 使用预训练数据来帮助finetune

## 2022-01

- [IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages](https://arxiv.org/abs/2201.11732)
  - A benchmark for transfer learning in NLP
  - 一个用于NLP跨模态、任务、语言的benchmark

- [Domain generalization in deep learning-based mass detection in mammography: A large-scale multi-center study](https://arxiv.org/abs/2201.11620)
  - Domain generalization in mass detection in mammography
  - Domain generalization进行胸部射线检测

- [Domain-Invariant Representation Learning from EEG with Private Encoders](https://arxiv.org/abs/2201.11613)
  - Domain-invariant learning from EEG
  - 用于EEG信号的领域不变特征研究

- [Gap Minimization for Knowledge Sharing and Transfer](https://arxiv.org/abs/2201.11231)
  - Multitask learning with gap minimization
  - 用于多任务学习的gap minimization方法

- [DROPO: Sim-to-Real Transfer with Offline Domain Randomization](https://arxiv.org/abs/2201.08434)
  - Sim-to-real transfer with domain randomization
  - 用domain randomization进行sim-to-real transfer

- AAAI-22 [Knowledge Sharing via Domain Adaptation in Customs Fraud Detection](https://arxiv.org/abs/2201.06759)
  - Domain adaptation for fraud detection
  - 用领域自适应进行欺诈检测

- [Continual Coarse-to-Fine Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2201.06974)
  - Domain adaptation in semantic segmentation
  - 领域自适应在语义分割的应用

- KBS-22 [Intra-domain and cross-domain transfer learning for time series data -- How transferable are the features](https://arxiv.org/abs/2201.04449)
  - An overview of transfer learning for time series data
  - 一个用迁移学习进行时间序列分析的小综述

- [A Likelihood Ratio based Domain Adaptation Method for E2E Models](2201.03655)
  - Domain adaptation for speech recognition
  - 用domain adaptation进行语音识别

- [Transfer Learning for Scene Text Recognition in Indian Languages](2201.03180)
  - Transfer learning for scene text recognition in Indian languages
  - 用迁移学习进行印度语的场景文字识别

- IEEE TMM-22 [Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement](https://arxiv.org/abs/2201.01929)
  - Invariant and shared components for Faster RCNN detection
  - 解耦公共和私有表征进行目标检测

- [Mixture of basis for interpretable continual learning with distribution shifts](https://arxiv.org/abs/2201.01853)
  - Incremental learning with mixture of basis
  - 用mixture of domains进行增量学习

- TKDE-22 [Adaptive Memory Networks with Self-supervised Learning for Unsupervised Anomaly Detection](https://arxiv.org/abs/2201.00464)
  - Adaptiev memory network for anomaly detection
  - 自适应的记忆网络用于异常检测

- ICIP-22 [Meta-Learned Feature Critics for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2112.13538)
  - Meta-learning for domain generalization
  - 元学习用于domain generalization

- ICIP-22 [Few-Shot Classification in Unseen Domains by Episodic Meta-Learning Across Visual Domains](https://arxiv.org/abs/2112.13539)
  - Few-shot generalization using meta-learning
  - 用元学习进行小样本的泛化

- [Data-Free Knowledge Transfer: A Survey](https://arxiv.org/abs/2112.15278)
  - A survey on data-free distillation and source-free DA
  - 一篇关于data-free蒸馏和source-free DA的综述

- [An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification](https://arxiv.org/abs/2112.13236)
  - An ensemble of pre-trained transformer for malware classification
  - 预训练的transformer通过集成进行恶意软件检测

- [Optimal Representations for Covariate Shift](https://arxiv.org/abs/2201.00057)
  - Learning optimal representations for covariate shift
  - 为covariate shift学习最优的表达

- [Transfer-learning-based Surrogate Model for Thermal Conductivity of Nanofluids](https://arxiv.org/abs/2201.00435)
  - Transfer learning for thermal conductivity
  - 迁移学习用于热传导

- [Transfer learning of phase transitions in percolation and directed percolation](https://arxiv.org/abs/2112.15516)
  - Transfer learning of phase transitions in percolation and directed percolation
  - 迁移学习用于precolation

- [Transfer learning for cancer diagnosis in histopathological images](https://arxiv.org/abs/2112.15523)
  - Transfer learning for cancer diagnosis
  - 迁移学习用于癌症诊断

## 2021-12

- IEEE TASLP-22 [Exploiting Adapters for Cross-lingual Low-resource Speech Recognition](https://arxiv.org/abs/2105.11905)  [Zhihu article](https://zhuanlan.zhihu.com/p/448216624)
    - Cross-lingual speech recogntion using meta-learning and transfer learning
    - 用元学习和迁移学习进行跨语言的低资源语音识别

- [More is Better: A Novel Multi-view Framework for Domain Generalization](https://arxiv.org/abs/2112.12329)
    - Multi-view learning for domain generalization
    - 使用多视图学习来进行domain generalization

- [SLIP: Self-supervision meets Language-Image Pre-training](https://arxiv.org/abs/2112.12750)
    - Self-supervised learning + language image pretraining
    - 用自监督学习用于语言到图像的预训练

- [Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems](https://arxiv.org/abs/2112.08718)
  - Prompt for domain adaptation in speech recognition
  - 用Prompt在语音识别中进行domain adaptation

- [UMAD: Universal Model Adaptation under Domain and Category Shift](https://arxiv.org/abs/2112.08553)
  - Model adaptation under domain and category shift
  - 在domain和class都有shift的前提下进行模型适配

- [Domain Adaptation on Point Clouds via Geometry-Aware Implicits](https://arxiv.org/abs/2112.09343)
  - Domain adaptation for point cloud
  - 针对点云的domain adaptation

- [A Survey of Unsupervised Domain Adaptation for Visual Recognition](http://arxiv.org/abs/2112.06745)
  - A new survey article of domain adaptation
  - 对UDA的一个综述文章，来自作者博士论文

- [VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks](http://arxiv.org/abs/2112.06825)
  - Vision-language efficient transfer learning
  - 参数高校的vision-language任务迁移

- [Federated Learning with Adaptive Batchnorm for Personalized Healthcare](https://arxiv.org/abs/2112.00734)
  - Federated learning with adaptive batchnorm
  - 用自适应BN进行个性化联邦学习

- [Unsupervised Domain Adaptation: A Reality Check](https://arxiv.org/abs/2111.15672)
  - Doing experiments to show the progress of DA methods over the years
  - 用大量的实验来验证近几年来DA方法的进展

- [Hierarchical Optimal Transport for Unsupervised Domain Adaptation](https://arxiv.org/abs/2112.02073)
  - hierarchical optimal transport for UDA
  - 层次性的最优传输用于domain adaptation

- [Unsupervised Domain Generalization by Learning a Bridge Across Domains](https://arxiv.org/abs/2112.02300)
  - Unsupervised domain generalization
  - 无监督的domain generalization

- [Boosting Unsupervised Domain Adaptation with Soft Pseudo-label and Curriculum Learning](https://arxiv.org/abs/2112.01948)
  - Using soft pseudo-label and curriculum learning to boost UDA
  - 用软的伪标签和课程学习增强UDA方法

- [Subtask-dominated Transfer Learning for Long-tail Person Search](https://arxiv.org/abs/2112.00527)
  - Subtask-dominated transfer for long-tail person search
  - 子任务驱动的长尾人物搜索

- [Revisiting the Transferability of Supervised Pretraining: an MLP Perspective](https://arxiv.org/abs/2112.00496)
  - Revisit the transferability of supervised pretraining
  - 重新思考有监督预训练的可迁移性

- [Multi-Agent Transfer Learning in Reinforcement Learning-Based Ride-Sharing Systems](https://arxiv.org/abs/2112.00424)
  - Multi-agent transfer in RL
  - 在RL中的多智能体迁移

- NeurIPS-21 [On Learning Domain-Invariant Representations for Transfer Learning with Multiple Sources](https://arxiv.org/abs/2111.13822)
  - Theory and algorithm of domain-invariant learning for transfer learning
  - 对invariant representation的理论和算法

- WACV-22 [Semi-supervised Domain Adaptation via Sample-to-Sample Self-Distillation](https://arxiv.org/abs/2111.14353)
  - Sample-level self-distillation for semi-supervised DA
  - 样本层次的自蒸馏用于半监督DA

- [ROBIN : A Benchmark for Robustness to Individual Nuisancesin Real-World Out-of-Distribution Shifts](https://arxiv.org/abs/2111.14341)
  - A benchmark for robustness to individual OOD
  - 一个OOD的benchmark

- ICML-21 workshop [Towards Principled Disentanglement for Domain Generalization](https://arxiv.org/abs/2111.13839)
  - Principled disentanglement for domain generalization
  - Principled解耦用于domain generalization

## 2021-11

  - NeurIPS-21 workshop [CytoImageNet: A large-scale pretraining dataset for bioimage transfer learning](https://arxiv.org/abs/2111.11646)
    - A large-scale dataset for bioimage transfer learning
    - 一个大规模的生物图像数据集用于迁移学习

  - NeurIPS-21 workshop [Component Transfer Learning for Deep RL Based on Abstract Representations](https://arxiv.org/abs/2111.11525)
    - Deep transfer learning for RL
    - 深度迁移学习用于强化学习

  - NeurIPS-21 workshop [Maximum Mean Discrepancy for Generalization in the Presence of Distribution and Missingness Shift](https://arxiv.org/abs/2111.10344)
    - MMD for covariate shift
    - 用MMD来解决covariate shift问题

  - [Combined Scaling for Zero-shot Transfer Learning](https://arxiv.org/abs/2111.10050)
    - Scaling up for zero-shot transfer learning
    - 增大训练规模用于zero-shot迁移学习

  - [Federated Learning with Domain Generalization](https://arxiv.org/abs/2111.10487)
    - Federated domain generalization
    - 联邦学习+domain generalization

  - [Semi-Supervised Domain Generalization in Real World:New Benchmark and Strong Baseline](https://arxiv.org/abs/2111.10221)
    - Semi-supervised domain generalization
    - 半监督+domain generalization

  - MICCAI-21 [Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning](https://arxiv.org/abs/2111.10827)
    - Domain generalization for mammography detection
    - 领域泛化用于乳房X射线检查

  - [On Representation Knowledge Distillation for Graph Neural Networks](https://arxiv.org/abs/2111.04964)
    - Knowledge distillation for GNN
    - 适用于GNN的知识蒸馏

  - BMVC-21 [Domain Attention Consistency for Multi-Source Domain Adaptation](https://arxiv.org/abs/2111.03911)
    - Multi-source domain adaptation using attention consistency
    - 用attention一致性进行多源的domain adaptation

  - [Action Recognition using Transfer Learning and Majority Voting for CSGO](https://arxiv.org/abs/2111.03882)
    - Using transfer learning and majority voting for action recognition
    - 使用迁移学习和多数投票进行动作识别

  - [Open-Set Crowdsourcing using Multiple-Source Transfer Learning](https://arxiv.org/abs/2111.04073)
    - Open-set crowdsourcing using multiple-source transfer learning
    - 使用多源迁移进行开放集的crowdsourcing

  - [Improved Regularization and Robustness for Fine-tuning in Neural Networks](https://arxiv.org/abs/2111.04578)
    - Improve regularization and robustness for finetuning
    - 针对finetune提高其正则和鲁棒性

  - [TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation](https://arxiv.org/abs/2111.02682)
    - Temporal domain adaptation

  - NeurIPS-21 [Modular Gaussian Processes for Transfer Learning](https://arxiv.org/abs/2110.13515)
    - Modular Gaussian process for transfer learning
    - 在迁移学习中使用modular Gaussian过程

  - [Estimating and Maximizing Mutual Information for Knowledge Distillation](https://arxiv.org/abs/2110.15946)
    - Global and local mutual information maximation for knowledge distillation
    - 局部和全局互信息最大化用于蒸馏

  - [On Label Shift in Domain Adaptation via Wasserstein Distance](https://arxiv.org/abs/2110.15520)
    - Using Wasserstein distance to solve label shift in domain adaptation
    - 在DA领域中用Wasserstein distance去解决label shift问题

  - [Xi-Learning: Successor Feature Transfer Learning for General Reward Functions](https://arxiv.org/abs/2110.15701)
    - General reward function transfer learning in RL
    - 在强化学习中general reward function的迁移学习

  - [C-MADA: Unsupervised Cross-Modality Adversarial Domain Adaptation framework for medical Image Segmentation](https://arxiv.org/abs/2110.15823)
    - Cross-modality domain adaptation for medical image segmentation
    - 跨模态的DA用于医学图像分割

  - [Deep Transfer Learning for Multi-source Entity Linkage via Domain Adaptation](https://arxiv.org/abs/2110.14509)
    - Domain adaptation for multi-source entiry linkage
    - 用DA进行多源的实体链接

  - [Temporal Knowledge Distillation for On-device Audio Classification](https://arxiv.org/abs/2110.14131)
    - Temporal knowledge distillation for on-device ASR
    - 时序知识蒸馏用于设备端的语音识别

  - [Transferring Domain-Agnostic Knowledge in Video Question Answering](https://arxiv.org/abs/2110.13395)
    - Domain-agnostic learning for VQA
    - 在VQA任务中进行迁移学习

## 2021-10

  - BMVC-21 [SILT: Self-supervised Lighting Transfer Using Implicit Image Decomposition](https://arxiv.org/abs/2110.12914)
    - Lighting transfer using implicit image decomposition
    - 用隐式图像分解进行光照迁移

  - [Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval](https://arxiv.org/abs/2110.12812)
    - Domain adaptation for cross-modal video retrieval
    - 用领域自适应进行跨模态的视频检索

  - [Age and Gender Prediction using Deep CNNs and Transfer Learning](https://arxiv.org/abs/2110.12633)
    - Age and gender prediction using transfer learning
    - 用迁移学习进行年龄和性别预测

  - [Domain Adaptation for Rare Classes Augmented with Synthetic Samples](https://arxiv.org/abs/2110.12216)
    - Domain adaptation for rare class
    - 稀疏类的domain adaptation

  - WACV-22 [AuxAdapt: Stable and Efficient Test-Time Adaptation for Temporally Consistent Video Semantic Segmentation](https://arxiv.org/abs/2110.12369)
    - Test-time adaptation for video semantic segmentation
    - 测试时adaptation用于视频语义分割

  - NeurIPS-21 [Unsupervised Domain Adaptation with Dynamics-Aware Rewards in Reinforcement Learning](https://arxiv.org/abs/2110.12997)
    - Domain adaptation in reinforcement learning
    - 在强化学习中应用domain adaptation

  - WACV-21 [Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition](https://arxiv.org/abs/2110.10101)
      - Domain generalization by audio-visual alignment
      - 通过音频-视频对齐进行domain generalization

  - BMVC-21 [Dynamic Feature Alignment for Semi-supervised Domain Adaptation](https://arxiv.org/abs/2110.09641)
    - Dynamic feature alignment for semi-supervised DA
    - 动态特征对齐用于半监督DA

  - NeurIPS-21 [FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling](https://arxiv.org/abs/2110.08263) [知乎解读](https://zhuanlan.zhihu.com/p/422930830) [code](https://github.com/TorchSSL/TorchSSL)
    - Curriculum pseudo label with a unified codebase TorchSSL
    - 半监督方法FlexMatch和统一算法库TorchSSL

  - [Rethinking supervised pre-training for better downstream transferring](https://arxiv.org/abs/2110.06014)
    - Rethink better finetune
    - 重新思考预训练以便更好finetune

  - [Music Sentiment Transfer](https://arxiv.org/abs/2110.05765)
    - Music sentiment transfer learning
    - 迁移学习用于音乐sentiment

  - NeurIPS-21 [Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data](http://arxiv.org/abs/2110.03374)
    - Source-free domain adaptation using constrastive learning
    - 无源域数据的DA，利用对比学习

  - [Understanding Domain Randomization for Sim-to-real Transfer](http://arxiv.org/abs/2110.03239)
    - Understanding domain randomizationfor sim-to-real transfer
    - 对强化学习中的sim-to-real transfer进行理论上的分析

  - [Dynamically Decoding Source Domain Knowledge For Unseen Domain Generalization](http://arxiv.org/abs/2110.03027)
    - Ensemble learning for domain generalization
    - 用集成学习进行domain generalization

  - [Scale Invariant Domain Generalization Image Recapture Detection](http://arxiv.org/abs/2110.03496)
    - Scale invariant domain generalizaiton
    - 尺度不变的domain generalization

## 2021-09

  - IEEE TIP-21 [Joint Clustering and Discriminative Feature Alignment for Unsupervised Domain Adaptation](https://ieeexplore.ieee.org/abstract/document/9535218)
    - Clustering and discriminative alignment for DA
    - 聚类与判定式对齐用于DA

  - IEEE TNNLS-21 [Entropy Minimization Versus Diversity Maximization for Domain Adaptation](https://ieeexplore.ieee.org/abstract/document/9537640)
    - Entropy minimization versus diversity max for DA
    - 熵最小化与diversity最大化

  - [Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation](https://arxiv.org/abs/2109.11798)
    - Adversarial domain adaptation for bronchoscopic depth estimation
    - 用对抗领域自适应进行支气管镜的深度估计

  - EMNLP-21 [Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning](https://arxiv.org/abs/2109.06349)
    - Few-shot intent detection using pretrain and finetune
    - 用迁移学习进行少样本意图检测

  - EMNLP-21 [Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation](https://arxiv.org/abs/2109.06604)
    - UDA for machine translation
    - 用领域自适应进行机器翻译

  - [KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation](https://arxiv.org/abs/2109.06243)
    - Using Kronecker decomposition and knowledge distillation for pre-trained language models compression
    - 用Kronecker分解和知识蒸馏来进行语言模型的压缩

  - [Cross-Region Domain Adaptation for Class-level Alignment](https://arxiv.org/abs/2109.06422)
    - Cross-region domain adaptation for class-level alignment
    - 跨区域的领域自适应用于类级别的对齐

  - [Unsupervised domain adaptation for cross-modality liver segmentation via joint adversarial learning and self-learning](https://arxiv.org/abs/2109.05664)
    - Domain adaptation for cross-modality liver segmentation
    - 使用domain adaptation进行肝脏的跨模态分割

  - [CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2109.06165)
    - Cross-domain transformer for domain adaptation
    - 基于transformer进行domain adaptation

  - ICCV-21 [Shape-Biased Domain Generalization via Shock Graph Embeddings](https://arxiv.org/abs/2109.05671)
    - Domain generalization based on shape information
    - 基于形状进行domain generalization

  - [Domain and Content Adaptive Convolution for Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2109.05676)
    - Domain generalization for medical image segmentation
    - 领域泛化用于医学图像分割

  - [Class-conditioned Domain Generalization via Wasserstein Distributional Robust Optimization](https://arxiv.org/abs/2109.03676)
    - Domain generalization with wasserstein DRO
    - 使用Wasserstein DRO进行domain generalization

  - [FedZKT: Zero-Shot Knowledge Transfer towards Heterogeneous On-Device Models in Federated Learning](https://arxiv.org/abs/2109.03775)
    - Zero-shot transfer in heterogeneous federated learning
    - 零次迁移用于联邦学习

  - [Fishr: Invariant Gradient Variances for Out-of-distribution Generalization](https://arxiv.org/abs/2109.02934)
    - Invariant gradient variances for OOD generalization
    - 不变梯度方差，用于OOD

  - [How Does Adversarial Fine-Tuning Benefit BERT?](https://arxiv.org/abs/2108.13602)
    - Examine how does adversarial fine-tuning help BERT
    - 探索对抗性finetune如何帮助BERT

  - [Contrastive Domain Adaptation for Question Answering using Limited Text Corpora](https://arxiv.org/abs/2108.13854)
    - Contrastive domain adaptation for QA
    - QA任务中应用对比domain adaptation

## 2021-08

  - [Robust Ensembling Network for Unsupervised Domain Adaptation](https://arxiv.org/abs/2108.09473)
    - Ensembling network for domain adaptation
    - 集成嵌入网络用于domain adaptation

  - [Federated Multi-Task Learning under a Mixture of Distributions](https://arxiv.org/abs/2108.10252)
    - Federated multi-task learning
    - 联邦多任务学习

  - [Fine-tuning is Fine in Federated Learning](http://arxiv.org/abs/2108.07313)
    - Finetuning in federated learning
    - 在联邦学习中进行finetune

  - [Federated Multi-Target Domain Adaptation](http://arxiv.org/abs/2108.07792)
    - Federated multi-target DA
    - 联邦学习场景下的多目标DA

  - [Learning Transferable Parameters for Unsupervised Domain Adaptation](https://arxiv.org/abs/2108.06129)
    - Learning partial transfer parameters for DA
    - 学习适用于迁移部分的参数做UDA任务

  - MICCAI-21 [A Systematic Benchmarking Analysis of Transfer Learning for Medical Image Analysis](https://arxiv.org/abs/2108.05930)
    - A benchmark of transfer learning for medical image
    - 一个详细的迁移学习用于医学图像的benchmark

  - [TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2108.05988)
    - Vision transformer for domain adaptation
    - 用视觉transformer进行DA

  - CIKM-21 [AdaRNN: Adaptive Learning and Forecasting of Time Series](https://arxiv.org/abs/2108.04443) [Code](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn) [知乎文章](https://zhuanlan.zhihu.com/p/398036372) [Video](https://www.bilibili.com/video/BV1Gh411B7rj/)
    - A new perspective to using transfer learning for time series analysis
    - 一种新的建模时间序列的迁移学习视角

  - TKDE-21 [Unsupervised Deep Anomaly Detection for Multi-Sensor Time-Series Signals](https://arxiv.org/abs/2107.12626)
    - Anomaly detection using semi-supervised and transfer learning
    - 半监督学习用于无监督异常检测

  - SemDIAL-21 [Generating Personalized Dialogue via Multi-Task Meta-Learning](https://arxiv.org/abs/2108.03377)
    - Generate personalized dialogue using multi-task meta-learning
    - 用多任务元学习生成个性化的对话

  - ICCV-21 [BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation](https://arxiv.org/abs/2108.03267)
    - Bijective MMD for domain adaptation
    - 双射MMD用于语义分割

  - [A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions](https://arxiv.org/abs/2108.03357)
    - A survey on cross-domain recommendation
    - 跨领域的推荐的综述

  - [A Data Augmented Approach to Transfer Learning for Covid-19 Detection](https://arxiv.org/abs/2108.02870)
    - Data augmentation to transfer learning for COVID
    - 迁移学习使用数据增强，用于COVID-19

  - MM-21 [Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding](https://arxiv.org/abs/2108.02953)
    - Few-shot DA with image-to-class sparse similarity encoding
    - 小样本的领域自适应

  - [Dual-Tuning: Joint Prototype Transfer and Structure Regularization for Compatible Feature Learning](https://arxiv.org/abs/2108.02959)
    - Prototype transfer and structure regularization
    - 原型的迁移学习

  - [Finetuning Pretrained Transformers into Variational Autoencoders](https://arxiv.org/abs/2108.02446)
    - Finetune transformer to VAE
    - 把transformer迁移到VAE

  - [Pre-trained Models for Sonar Images](http://arxiv.org/abs/2108.01111)
    - Pre-trained models for sonar images
    - 针对声纳图像的预训练模型

  - [Domain Adaptor Networks for Hyperspectral Image Recognition](http://arxiv.org/abs/2108.01555)
    - Finetune for hyperspectral image recognition
    - 针对高光谱图像识别的迁移学习

## 2021-07

  - CVPR-21 [Efficient Conditional GAN Transfer With Knowledge Propagation Across Classes](https://openaccess.thecvf.com/content/CVPR2021/html/Shahbazi_Efficient_Conditional_GAN_Transfer_With_Knowledge_Propagation_Across_Classes_CVPR_2021_paper.html)
    - Transfer conditional GANs to unseen classes
    - 通过知识传递，迁移预训练的conditional GAN到新类别

  - CVPR-21 [Ego-Exo: Transferring Visual Representations From Third-Person to First-Person Videos](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.html)
    - Transfer learning from third-person to first-person video
    - 从第三人称视频迁移到第一人称

  - [Toward Co-creative Dungeon Generation via Transfer Learning](http://arxiv.org/abs/2107.12533)
    - Game scene generation with transfer learning
    - 用迁移学习生成游戏场景

  - [Transfer Learning in Electronic Health Records through Clinical Concept Embedding](https://arxiv.org/abs/2107.12919)
    - Transfer learning in electronic health record
    - 迁移学习用于医疗记录管理

  - CVPR-21 [Conditional Bures Metric for Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Conditional_Bures_Metric_for_Domain_Adaptation_CVPR_2021_paper.html)
    - A new metric for domain adaptation
    - 提出一个新的metric用于domain adaptation

  - CVPR-21 [Wasserstein Barycenter for Multi-Source Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/html/Montesuma_Wasserstein_Barycenter_for_Multi-Source_Domain_Adaptation_CVPR_2021_paper.html)
    - Use Wasserstein Barycenter for multi-source domain adaptation
    - 利用Wasserstein Barycenter进行DA

  - CVPR-21 [Generalized Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/html/Mitsuzumi_Generalized_Domain_Adaptation_CVPR_2021_paper.html)
    - A general definition for domain adaptation
    - 一个更抽象更一般的domain adaptation定义

  - CVPR-21 [Reducing Domain Gap by Reducing Style Bias](https://openaccess.thecvf.com/content/CVPR2021/html/Nam_Reducing_Domain_Gap_by_Reducing_Style_Bias_CVPR_2021_paper.html)
    - Syle-invariant training for adaptation and generalization
    - 通过训练图像对style无法辨别来进行DA和DG

  - CVPR-21 [Uncertainty-Guided Model Generalization to Unseen Domains](https://openaccess.thecvf.com/content/CVPR2021/html/Qiao_Uncertainty-Guided_Model_Generalization_to_Unseen_Domains_CVPR_2021_paper.html)
    - Uncertainty-guided generalization
    - 基于不确定性的domain generalization

  - CVPR-21 [Adaptive Methods for Real-World Domain Generalization](https://openaccess.thecvf.com/content/CVPR2021/html/Dubey_Adaptive_Methods_for_Real-World_Domain_Generalization_CVPR_2021_paper.html)
    - Adaptive methods for domain generalization
    - 动态算法，用于domain generalization

  - 20210716 ICML-21 [Continual Learning in the Teacher-Student Setup: Impact of Task Similarity](https://arxiv.org/abs/2107.04384)
    - Investigating task similarity in teacher-student learning
    - 调研在continual learning下teacher-student learning问题的任务相似度

  - 20210716 BMCV-extend [Exploring Dropout Discriminator for Domain Adaptation](https://arxiv.org/abs/2107.04231)
    - Using multiple discriminators for domain adaptation
    - 用分布估计代替点估计来做domain adaptation

  - 20210716 TPAMI-21 [Lifelong Teacher-Student Network Learning](https://arxiv.org/abs/2107.04689)
    - Lifelong distillation
    - 持续的知识蒸馏

  - 20210716 MICCAI-21 [Few-Shot Domain Adaptation with Polymorphic Transformers](https://arxiv.org/abs/2107.04805)
    - Few-shot domain adaptation with polymorphic transformer
    - 用多模态transformer做少样本的domain adaptation

  - 20210716 InterSpeech-21 [Speech2Video: Cross-Modal Distillation for Speech to Video Generation](https://arxiv.org/abs/2107.04806)
    - Cross-model distillation for video generation
    - 跨模态蒸馏用于语音到video的生成

  - 20210716 ICML-21 workshop [Leveraging Domain Adaptation for Low-Resource Geospatial Machine Learning](https://arxiv.org/abs/2107.04983)
    - Using domain adaptation for geospatial ML
    - 用domain adaptation进行地理空间的机器学习
